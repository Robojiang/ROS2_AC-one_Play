#!/usr/bin/env python3
# -*- coding:utf-8 -*-

"""
å°†HDF5æ•°æ®é›†è½¬æ¢ä¸ºZarræ ¼å¼,åŒ…å«ç‚¹äº‘ç”Ÿæˆ
ç”¨æ³•: python convert_hdf5_to_zarr.py --num_episodes 100 --max_episodes 5 (debugæ¨¡å¼)
"""

import os
import h5py
import zarr
import numpy as np
import cv2
import json
import argparse
import shutil
from tqdm import tqdm
from scipy.spatial.transform import Rotation as R
import open3d as o3d
import torch
import torch.nn.functional as F

# ================= é…ç½® =================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(BASE_DIR)  # ä¸Šä¸€çº§ç›®å½•
DATASETS_DIR = os.path.join(BASE_DIR, "datasets")
DATASETS_ZARR_DIR = os.path.join(BASE_DIR, "datasets_zarr")
CALIBRATION_DIR = os.path.join(BASE_DIR, "calibration_results")
INTRINSICS_FILE = os.path.join(CALIBRATION_DIR, "D405_intrinsics.json")

# ç‚¹äº‘é…ç½®
MAX_DEPTH_Head = 1.0  # ç±³
MAX_DEPTH_Hand = 0.6  # ç±³
FPS_SAMPLE_POINTS = 2048  # ç‚¹äº‘é‡‡æ ·ç‚¹æ•°
USE_RANDOM_SAMPLING = True  # True: éšæœºé‡‡æ ·(å¿«), False: FPSé‡‡æ ·(æ…¢ä½†å‡åŒ€)

# âš¡ GPUåŠ é€Ÿé…ç½®
USE_GPU = torch.cuda.is_available()
DEVICE = torch.device('cuda' if USE_GPU else 'cpu')
DOWNSAMPLE_SIZE = (160, 120)  # ä»640x480é™é‡‡æ ·åˆ°160x120 (16å€é™é‡‡æ ·)

# å·¥ä½œç©ºé—´è£å‰ª (ç›¸å¯¹äºå·¦è‡‚åŸºåº§åæ ‡ç³»)
USE_WORKSPACE_CROP = True
WORKSPACE_X_RANGE = [-0.4, 0.5]
WORKSPACE_Y_RANGE = [-0.5, 3.0]
WORKSPACE_Z_RANGE = [-0.2, 1.0]

# å…³é”®å¸§æ£€æµ‹
GRIPPER_DELTA = 0.05  # å¤¹çˆªå˜åŒ–é˜ˆå€¼
MIN_INTERVAL = 20  # æœ€å°å…³é”®å¸§é—´éš”

print(f"âš¡ GPUåŠ é€Ÿ: {'å¯ç”¨' if USE_GPU else 'ç¦ç”¨ (ä½¿ç”¨CPU)'}")
if USE_GPU:
    print(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"   é™é‡‡æ ·å°ºå¯¸: {DOWNSAMPLE_SIZE[0]}x{DOWNSAMPLE_SIZE[1]} (åŸå§‹: 640x480)")
print(f"   é‡‡æ ·æ–¹å¼: {'éšæœºé‡‡æ · (å¿«é€Ÿ)' if USE_RANDOM_SAMPLING else 'FPSé‡‡æ · (æ…¢ä½†å‡åŒ€)'}")


# ================= GPUåŠ é€Ÿç‚¹äº‘ç”Ÿæˆå™¨ =================

class GPUPointCloudGenerator:
    """
    GPUåŠ é€Ÿç‚¹äº‘ç”Ÿæˆå™¨
    æ ¸å¿ƒä¼˜åŒ–:
    1. é¢„è®¡ç®—æŠ•å½±çŸ©é˜µ (u-cx)/fx, (v-cy)/fy
    2. å…ˆé™é‡‡æ ·å›¾åƒå†ç”Ÿæˆç‚¹äº‘ (160x120 vs 640x480)
    3. å…¨æµç¨‹åœ¨GPUä¸Šç”¨PyTorchå®Œæˆ
    """
    
    def __init__(self, intrinsics, downsample_size=(160, 120), device='cuda'):
        """
        Args:
            intrinsics: dict with keys 'head', 'left', 'right', each containing (fx, fy, cx, cy)
            downsample_size: (width, height) for downsampling
            device: 'cuda' or 'cpu'
        """
        self.device = torch.device(device)
        self.downsample_size = downsample_size
        self.w, self.h = downsample_size
        
        # é¢„è®¡ç®—æ¯ä¸ªç›¸æœºçš„å°„çº¿æ–¹å‘çŸ©é˜µ
        self.ray_dirs = {}
        for cam_name, (fx, fy, cx, cy) in intrinsics.items():
            # è°ƒæ•´å†…å‚åˆ°é™é‡‡æ ·åˆ†è¾¨ç‡
            # å‡è®¾åŸå§‹åˆ†è¾¨ç‡æ˜¯ 640x480
            scale_x = self.w / 640.0
            scale_y = self.h / 480.0
            fx_scaled = fx * scale_x
            fy_scaled = fy * scale_y
            cx_scaled = cx * scale_x
            cy_scaled = cy * scale_y
            
            # ç”Ÿæˆåƒç´ åæ ‡ç½‘æ ¼
            u, v = torch.meshgrid(
                torch.arange(self.w, device=self.device, dtype=torch.float32),
                torch.arange(self.h, device=self.device, dtype=torch.float32),
                indexing='xy'
            )
            
            # è®¡ç®—å°„çº¿æ–¹å‘ (é¢„è®¡ç®—)
            x_over_z = (u - cx_scaled) / fx_scaled  # (H, W)
            y_over_z = (v - cy_scaled) / fy_scaled
            
            self.ray_dirs[cam_name] = (x_over_z, y_over_z)
        
        print(f"[GPUç‚¹äº‘ç”Ÿæˆå™¨] åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}, é™é‡‡æ ·: {self.w}x{self.h}")
    
    def depth_to_pointcloud(self, depth_img, color_img, cam_name, max_depth=None):
        """
        å°†æ·±åº¦å›¾å’Œå½©è‰²å›¾è½¬æ¢ä¸ºç‚¹äº‘ (GPUç‰ˆæœ¬)
        
        Args:
            depth_img: (H, W) numpy array, uint16, mm
            color_img: (H, W, 3) numpy array, uint8, RGB
            cam_name: 'head', 'left', 'right'
            max_depth: æœ€å¤§æ·±åº¦(ç±³)
        
        Returns:
            point_cloud: (N, 6) torch tensor on device, [x, y, z, r, g, b]
        """
        # 1. é™é‡‡æ ·
        depth_small = cv2.resize(depth_img, self.downsample_size, interpolation=cv2.INTER_NEAREST)
        color_small = cv2.resize(color_img, self.downsample_size, interpolation=cv2.INTER_LINEAR)
        
        # 2. è½¬æ¢ä¸ºtorch tensorå¹¶ç§»åˆ°GPU
        depth_t = torch.from_numpy(depth_small).to(self.device).float() / 1000.0  # mm -> m
        color_t = torch.from_numpy(color_small).to(self.device).float() / 255.0   # [0, 255] -> [0, 1]
        
        # 3. æœ‰æ•ˆæ€§æ©ç 
        valid = depth_t > 0
        if max_depth is not None:
            valid = valid & (depth_t < max_depth)
        
        # 4. ä½¿ç”¨é¢„è®¡ç®—çš„å°„çº¿æ–¹å‘
        x_over_z, y_over_z = self.ray_dirs[cam_name]
        
        # 5. è®¡ç®—3Dåæ ‡
        z = depth_t  # (H, W)
        x = x_over_z * z
        y = y_over_z * z
        
        # 6. å±•å¹³å¹¶è¿‡æ»¤æœ‰æ•ˆç‚¹
        x_flat = x[valid]  # (N,)
        y_flat = y[valid]
        z_flat = z[valid]
        
        # ä»RGBå›¾åƒæå–é¢œè‰² (å…ˆåˆ†ç¦»é€šé“å†åº”ç”¨æ©ç )
        r_flat = color_t[:, :, 0][valid]  # (N,)
        g_flat = color_t[:, :, 1][valid]
        b_flat = color_t[:, :, 2][valid]
        
        # 7. æ‹¼æ¥ä¸º (N, 6)
        xyz = torch.stack([x_flat, y_flat, z_flat], dim=1)  # (N, 3)
        rgb = torch.stack([r_flat, g_flat, b_flat], dim=1)  # (N, 3)
        
        return torch.cat([xyz, rgb], dim=1)  # (N, 6)
    
    def transform_pointcloud(self, cloud, T):
        """
        å˜æ¢ç‚¹äº‘ (GPUç‰ˆæœ¬)
        
        Args:
            cloud: (N, 6) tensor, [x, y, z, r, g, b]
            T: (4, 4) numpy array, transformation matrix
        
        Returns:
            transformed_cloud: (N, 6) tensor
        """
        T_t = torch.from_numpy(T).to(self.device).float()
        
        xyz = cloud[:, :3]  # (N, 3)
        rgb = cloud[:, 3:]  # (N, 3)
        
        # é½æ¬¡åæ ‡
        ones = torch.ones((xyz.shape[0], 1), device=self.device)
        xyz_homo = torch.cat([xyz, ones], dim=1)  # (N, 4)
        
        # å˜æ¢
        xyz_trans = (T_t @ xyz_homo.T).T  # (N, 4)
        
        return torch.cat([xyz_trans[:, :3], rgb], dim=1)  # (N, 6)
    
    def crop_pointcloud(self, cloud, x_range, y_range, z_range):
        """
        è£å‰ªç‚¹äº‘ (GPUç‰ˆæœ¬)
        
        Args:
            cloud: (N, 6) tensor
            x/y/z_range: [min, max]
        
        Returns:
            cropped_cloud: (M, 6) tensor
        """
        xyz = cloud[:, :3]
        mask = (
            (xyz[:, 0] >= x_range[0]) & (xyz[:, 0] <= x_range[1]) &
            (xyz[:, 1] >= y_range[0]) & (xyz[:, 1] <= y_range[1]) &
            (xyz[:, 2] >= z_range[0]) & (xyz[:, 2] <= z_range[1])
        )
        return cloud[mask]
    
    def generate_frame(self, head_depth, head_color, left_depth, left_color,
                      right_depth, right_color, left_eef, right_eef,
                      T_H_LB, T_H_RB, T_LE_LC, T_RE_RC, T_LB_H,
                      max_depth_head, max_depth_hand,
                      use_workspace_crop=True,
                      workspace_x_range=None, workspace_y_range=None, workspace_z_range=None):
        """
        ç”Ÿæˆå•å¸§ç‚¹äº‘ (GPUåŠ é€Ÿç‰ˆæœ¬)
        
        Returns:
            point_cloud: (FPS_SAMPLE_POINTS, 6) numpy array
        """
        clouds = []
        
        # 1. Head Camera
        pc_head = self.depth_to_pointcloud(head_depth, head_color, 'head', max_depth=max_depth_head)
        if len(pc_head) > 0:
            clouds.append(pc_head)
        
        # 2. Left Wrist Camera
        pc_left = self.depth_to_pointcloud(left_depth, left_color, 'left', max_depth=max_depth_hand)
        if len(pc_left) > 0:
            T_LB_LE = eef_to_matrix(left_eef)
            T_total_left = T_H_LB @ T_LB_LE @ T_LE_LC
            pc_left = self.transform_pointcloud(pc_left, T_total_left)
            clouds.append(pc_left)
        
        # 3. Right Wrist Camera
        pc_right = self.depth_to_pointcloud(right_depth, right_color, 'right', max_depth=max_depth_hand)
        if len(pc_right) > 0:
            T_RB_RE = eef_to_matrix(right_eef)
            T_total_right = T_H_RB @ T_RB_RE @ T_RE_RC
            pc_right = self.transform_pointcloud(pc_right, T_total_right)
            clouds.append(pc_right)
        
        if len(clouds) == 0:
            return np.zeros((FPS_SAMPLE_POINTS, 6), dtype=np.float32)
        
        # 4. åˆå¹¶å¹¶è½¬æ¢åˆ°å·¦è‡‚åŸºåº§åæ ‡ç³»
        merged = torch.cat(clouds, dim=0)
        merged = self.transform_pointcloud(merged, T_LB_H)
        
        # 5. å·¥ä½œç©ºé—´è£å‰ª
        if use_workspace_crop:
            merged = self.crop_pointcloud(merged, workspace_x_range, workspace_y_range, workspace_z_range)
        
        if len(merged) == 0:
            return np.zeros((FPS_SAMPLE_POINTS, 6), dtype=np.float32)
        
        # 6. è½¬å›CPUè¿›è¡ŒOpen3Dä¸‹é‡‡æ · (FPSåœ¨GPUä¸Šå®ç°å¤æ‚,ç”¨CPUä¹Ÿå¤Ÿå¿«)
        merged_cpu = merged.cpu().numpy()
        
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(merged_cpu[:, :3])
        pcd.colors = o3d.utility.Vector3dVector(merged_cpu[:, 3:])
        
        # å»å™ª
        pcd_clean, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
        
        # ä½“ç´ ä¸‹é‡‡æ ·
        pcd_voxel = pcd_clean.voxel_down_sample(voxel_size=0.005)
        
        # æœ€ç»ˆé‡‡æ ·åˆ°å›ºå®šç‚¹æ•°
        if USE_RANDOM_SAMPLING:
            # éšæœºé‡‡æ · (å¿«é€Ÿ)
            pts = np.asarray(pcd_voxel.points)
            clrs = np.asarray(pcd_voxel.colors)
            
            if len(pts) > FPS_SAMPLE_POINTS:
                # éšæœºé€‰æ‹©ç´¢å¼•
                indices = np.random.choice(len(pts), FPS_SAMPLE_POINTS, replace=False)
                pts = pts[indices]
                clrs = clrs[indices]
            
            result = np.hstack((pts, clrs)).astype(np.float32)
        else:
            # FPSé‡‡æ · (æ…¢ä½†å‡åŒ€)
            if len(pcd_voxel.points) > FPS_SAMPLE_POINTS:
                pcd_fps = pcd_voxel.farthest_point_down_sample(FPS_SAMPLE_POINTS)
            else:
                pcd_fps = pcd_voxel
            
            pts = np.asarray(pcd_fps.points)
            clrs = np.asarray(pcd_fps.colors)
            result = np.hstack((pts, clrs)).astype(np.float32)
        
        # Padåˆ°å›ºå®šå¤§å°
        if len(result) < FPS_SAMPLE_POINTS:
            padding = np.zeros((FPS_SAMPLE_POINTS - len(result), 6), dtype=np.float32)
            result = np.vstack((result, padding))
        
        return result

# ================= æ ‡å®šåŠ è½½å‡½æ•° =================

def load_intrinsics(camera_name):
    """åŠ è½½ç›¸æœºå†…å‚"""
    with open(INTRINSICS_FILE, 'r') as f:
        all_data = json.load(f)
    d = all_data[camera_name]
    return d['fx'], d['fy'], d['cx'], d['cy']

def load_calibration_matrix(filename):
    """åŠ è½½æ ‡å®šçŸ©é˜µ"""
    path = os.path.join(CALIBRATION_DIR, filename)
    if os.path.exists(path):
        if path.endswith('.npy'):
            return np.load(path)
        elif path.endswith('.txt'):
            return np.loadtxt(path)
    print(f"âŒ ç¼ºå°‘æ ‡å®šæ–‡ä»¶: {filename}")
    return np.eye(4)

# ================= ç‚¹äº‘ç”Ÿæˆå‡½æ•° =================

def eef_to_matrix(eef_pose):
    """å°†end-effector poseè½¬æ¢ä¸º4x4å˜æ¢çŸ©é˜µ"""
    if eef_pose is None or len(eef_pose) < 6:
        return np.eye(4)
    t = np.array(eef_pose[:3])
    r = R.from_euler('xyz', eef_pose[3:6]).as_matrix()
    T = np.eye(4)
    T[:3, :3] = r
    T[:3, 3] = t
    return T

# ================= HDF5æ•°æ®è¯»å– =================

def decode_jpeg(data):
    """è§£ç JPEGæ•°æ®"""
    img_bgr = cv2.imdecode(np.frombuffer(data, dtype=np.uint8), cv2.IMREAD_COLOR)
    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)  # è½¬æ¢ä¸ºRGB

def load_hdf5_episode(hdf5_path):
    """
    è¯»å–å•ä¸ªHDF5æ–‡ä»¶
    è¿”å›: dict with keys: eef, images (head, left_wrist, right_wrist), depths, qpos
    """
    with h5py.File(hdf5_path, 'r') as f:
        # è¯»å–end-effectoræ•°æ®
        eef_data = f['observations/eef'][()]  # (T, 14)
        qpos_data = f['observations/qpos'][()]  # (T, 14)
        
        # è¯»å–å›¾åƒ (JPEGç¼–ç )
        head_imgs = [decode_jpeg(d) for d in f['observations/images/head'][()]]
        left_imgs = [decode_jpeg(d) for d in f['observations/images/left_wrist'][()]]
        right_imgs = [decode_jpeg(d) for d in f['observations/images/right_wrist'][()]]
        
        # è¯»å–æ·±åº¦
        head_depths = f['observations/images_depth/head'][()]
        left_depths = f['observations/images_depth/left_wrist'][()]
        right_depths = f['observations/images_depth/right_wrist'][()]
        
        return {
            'eef': eef_data,
            'qpos': qpos_data,
            'head_images': np.array(head_imgs),
            'left_images': np.array(left_imgs),
            'right_images': np.array(right_imgs),
            'head_depths': head_depths,
            'left_depths': left_depths,
            'right_depths': right_depths,
        }

# ================= å…³é”®å¸§æ£€æµ‹ =================

def transform_right_endpose_to_left_base(right_eef_array, T_H_LB, T_H_RB):
    """
    å°†å³è‡‚æœ«ç«¯å§¿æ€ä»å³è‡‚åŸºåº§åæ ‡ç³»è½¬æ¢åˆ°å·¦è‡‚åŸºåº§åæ ‡ç³»
    right_eef_array: (N, 7) [x, y, z, rx, ry, rz, gripper]
    T_H_LB: (4, 4) Headåˆ°å·¦è‡‚åŸºåº§çš„å˜æ¢çŸ©é˜µ (æ³¨æ„: æ–‡ä»¶åhead_base_to_leftçš„å«ä¹‰)
    T_H_RB: (4, 4) Headåˆ°å³è‡‚åŸºåº§çš„å˜æ¢çŸ©é˜µ
    è¿”å›: (N, 7) åœ¨å·¦è‡‚åŸºåº§åæ ‡ç³»ä¸‹çš„å§¿æ€
    
    å˜æ¢é“¾: Head -> RightBase -> RightEnd, ç„¶åè½¬åˆ°LeftBase
    å³: T_LB_RE = T_H_LB @ T_H_RB @ T_RB_RE
    """
    N = len(right_eef_array)
    result = np.zeros_like(right_eef_array)
    
    for i in range(N):
        # æå–å³è‡‚æœ«ç«¯åœ¨å³è‡‚åŸºåº§ç³»ä¸‹çš„å§¿æ€
        T_RB_RE = eef_to_matrix(right_eef_array[i])
        
        # è½¬æ¢åˆ°å·¦è‡‚åŸºåº§ç³»: Head -> RightBase -> RightEnd, å†è½¬åˆ°LeftBase
        T_LB_RE = T_H_LB @ T_H_RB @ T_RB_RE
        
        # æå–ä½ç½®
        result[i, :3] = T_LB_RE[:3, 3]
        
        # æå–æ—‹è½¬(è½¬æ¢ä¸ºæ¬§æ‹‰è§’)
        rot_matrix = T_LB_RE[:3, :3]
        result[i, 3:6] = R.from_matrix(rot_matrix).as_euler('xyz')
        
        # å¤¹çˆªå€¼ä¸å˜
        result[i, 6] = right_eef_array[i, 6]
    
    return result

def get_keyframe_mask(eef_data, gripper_delta=0.05, min_interval=5):
    """
    ç”Ÿæˆå…³é”®å¸§mask (åªåŸºäºå¤¹çˆªå¼€åˆ,ä¸è€ƒè™‘æš‚åœ)
    eef_data: (T, 14) [left(7), right(7)]
    """
    T = len(eef_data)
    mask = np.zeros(T, dtype=bool)
    
    # æå–å¤¹çˆªçŠ¶æ€
    left_gripper = eef_data[:, 6]  # ç¬¬7ç»´
    right_gripper = eef_data[:, 13]  # ç¬¬14ç»´
    
    # è®¡ç®—å¤¹çˆªå˜åŒ–
    left_diff = np.abs(np.diff(left_gripper, prepend=left_gripper[0]))
    right_diff = np.abs(np.diff(right_gripper, prepend=right_gripper[0]))
    
    # ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§æ€»æ˜¯å…³é”®å¸§
    mask[0] = True
    mask[-1] = True
    
    last_keyframe_idx = 0
    for i in range(1, T - 1):
        # æ£€æŸ¥å¤¹çˆªæ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
        is_gripper_change = (left_diff[i] > gripper_delta) or (right_diff[i] > gripper_delta)
        
        # å¼ºåˆ¶æœ€å°é—´éš”
        if (i - last_keyframe_idx) > min_interval and is_gripper_change:
            mask[i] = True
            last_keyframe_idx = i
    
    return mask

# ================= ä¸»è½¬æ¢å‡½æ•° =================

def convert_task_to_zarr(task_name, task_dir, max_episodes=None):
    """
    å°†å•ä¸ªä»»åŠ¡çš„HDF5æ•°æ®è½¬æ¢ä¸ºZarræ ¼å¼
    
    Args:
        task_name: ä»»åŠ¡åç§° (æ–‡ä»¶å¤¹å)
        task_dir: ä»»åŠ¡æ–‡ä»¶å¤¹è·¯å¾„
        max_episodes: ç”¨äºdebug,åªè½¬æ¢å‰Nä¸ªepisode (Noneè¡¨ç¤ºè½¬æ¢å…¨éƒ¨)
    """
    # è‡ªåŠ¨æ‰«æHDF5æ–‡ä»¶
    print(f"\n{'='*80}")
    print(f"ğŸ¯ ä»»åŠ¡: {task_name}")
    print(f"{'='*80}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {task_dir}")
    
    hdf5_files = sorted([f for f in os.listdir(task_dir) if f.endswith('.hdf5')])
    print(f"ğŸ” æ‰¾åˆ° {len(hdf5_files)} ä¸ªHDF5æ–‡ä»¶")
    
    if len(hdf5_files) == 0:
        print(f"âš ï¸  ä»»åŠ¡ {task_name} æ²¡æœ‰HDF5æ–‡ä»¶,è·³è¿‡")
        return
    
    # è¾“å‡ºè·¯å¾„
    os.makedirs(DATASETS_ZARR_DIR, exist_ok=True)
    save_dir = os.path.join(DATASETS_ZARR_DIR, f"{task_name}.zarr")
    
    if os.path.exists(save_dir):
        print(f"âš ï¸  åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {save_dir}")
        shutil.rmtree(save_dir)
    
    # åˆ›å»ºZarræ ¹
    zarr_root = zarr.group(save_dir)
    zarr_data = zarr_root.create_group("data")
    zarr_meta = zarr_root.create_group("meta")
    
    # åŠ è½½æ ‡å®šæ•°æ®
    print("\nğŸ“ åŠ è½½æ ‡å®šæ–‡ä»¶...")
    T_LE_LC = load_calibration_matrix("left_eye_in_hand.npy")
    T_RE_RC = load_calibration_matrix("right_eye_in_hand.npy")
    T_LB_H = load_calibration_matrix("head_base_to_left_refined_icp.txt")
    T_RB_H = load_calibration_matrix("head_base_to_right_refined_icp.txt")
    
    if np.array_equal(T_LB_H, np.eye(4)):
        T_LB_H = load_calibration_matrix("head_base_to_left.npy")
    if np.array_equal(T_RB_H, np.eye(4)):
        T_RB_H = load_calibration_matrix("head_base_to_right.npy")
    
    # æ³¨æ„: æ–‡ä»¶åhead_base_to_leftå®é™…è¡¨ç¤º Head->LeftBase çš„å˜æ¢
    # æ–‡ä»¶åhead_base_to_rightå®é™…è¡¨ç¤º RightBase->Head çš„å˜æ¢ (éœ€è¦å–é€†å¾—åˆ°Head->RightBase)
    # å’Œpointcloud_from_hdf5.pyä¿æŒä¸€è‡´
    T_H_LB = T_LB_H
    T_H_RB = np.linalg.inv(T_RB_H)
    
    intrinsics = {
        'head': load_intrinsics('head'),
        'left': load_intrinsics('left'),
        'right': load_intrinsics('right')
    }
    print("âœ… æ ‡å®šæ–‡ä»¶åŠ è½½å®Œæˆ")
    
    # âš¡ åˆå§‹åŒ–GPUç‚¹äº‘ç”Ÿæˆå™¨
    pc_generator = GPUPointCloudGenerator(
        intrinsics=intrinsics,
        downsample_size=DOWNSAMPLE_SIZE,
        device=DEVICE
    )
    
    # åˆå§‹åŒ–Zarræ•°æ®é›†
    compressor = zarr.Blosc(cname="zstd", clevel=3, shuffle=1)
    zarr_datasets = {}
    
    total_count = 0
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    files_to_process = hdf5_files[:max_episodes] if max_episodes is not None else hdf5_files
    
    print(f"\nğŸ”„ å¼€å§‹è½¬æ¢ (å…± {len(files_to_process)} episodes)...")
    print(f"ç¬¬ä¸€ä¸ªæ–‡ä»¶: {files_to_process[0]}")
    if len(files_to_process) > 1:
        print(f"æœ€åä¸€ä¸ªæ–‡ä»¶: {files_to_process[-1]}")
    
    for hdf5_filename in tqdm(files_to_process, desc=f"Converting {task_name}"):
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        hdf5_path = os.path.join(task_dir, hdf5_filename)
        
        if not os.path.exists(hdf5_path):
            print(f"\nâš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {hdf5_path}")
            continue
        
        try:
            # è¯»å–HDF5æ•°æ®
            data = load_hdf5_episode(hdf5_path)
            eef_data = data['eef']
            qpos_data = data['qpos']
            
            T = len(eef_data)
            if T < 2:
                print(f"\nâš ï¸  {hdf5_filename} å¤ªçŸ­,è·³è¿‡")
                continue
            
            # åˆ†ç¦»å·¦å³è‡‚
            left_eef = eef_data[:, :7]
            right_eef = eef_data[:, 7:14]
            
            # ç”Ÿæˆç‚¹äº‘ (æ¯ä¸€å¸§) - æ˜¾ç¤ºå¸§çº§åˆ«è¿›åº¦
            point_clouds = []
            print(f"\n  ğŸ“Š {hdf5_filename}: ç”Ÿæˆ {T} å¸§ç‚¹äº‘...")
            for t in tqdm(range(T), desc=f"  Processing frames", leave=False, ncols=80):
                pc = pc_generator.generate_frame(
                    head_depth=data['head_depths'][t],
                    head_color=data['head_images'][t],
                    left_depth=data['left_depths'][t],
                    left_color=data['left_images'][t],
                    right_depth=data['right_depths'][t],
                    right_color=data['right_images'][t],
                    left_eef=left_eef[t],
                    right_eef=right_eef[t],
                    T_H_LB=T_H_LB,
                    T_H_RB=T_H_RB,
                    T_LE_LC=T_LE_LC,
                    T_RE_RC=T_RE_RC,
                    T_LB_H=T_LB_H,
                    max_depth_head=MAX_DEPTH_Head,
                    max_depth_hand=MAX_DEPTH_Hand,
                    use_workspace_crop=USE_WORKSPACE_CROP,
                    workspace_x_range=WORKSPACE_X_RANGE,
                    workspace_y_range=WORKSPACE_Y_RANGE,
                    workspace_z_range=WORKSPACE_Z_RANGE
                )
                point_clouds.append(pc)
            
            point_clouds = np.array(point_clouds)  # (T, 1024, 6)
            
            # ç»„ç»‡å›¾åƒ (4ä¸ªç›¸æœº: head, left, right, è¿˜éœ€è¦ä¸€ä¸ªfront?)
            # æ ¹æ®ç›®æ ‡æ ¼å¼: (T, 4, 240, 320, 3)
            # å‡è®¾æˆ‘ä»¬resizeåˆ°240x320 (å›¾åƒå·²ç»æ˜¯RGBæ ¼å¼)
            def resize_images(imgs):
                return np.array([cv2.resize(img, (320, 240)) for img in imgs])
            
            head_resized = resize_images(data['head_images'])
            left_resized = resize_images(data['left_images'])
            right_resized = resize_images(data['right_images'])
            
            # åˆ›å»º4ä¸ªç›¸æœºçš„å›¾åƒ (å¦‚æœåªæœ‰3ä¸ª,å¤åˆ¶ä¸€ä¸ª)
            images = np.stack([head_resized, head_resized, left_resized, right_resized], axis=1)  # (T, 4, 240, 320, 3)
            
            # è®¡ç®—å…³é”®å¸§mask
            keyframe_mask = get_keyframe_mask(eef_data, GRIPPER_DELTA, MIN_INTERVAL)
            
            # å‡†å¤‡episodeæ•°æ® (state[t] + action[t] -> state[t+1])
            ep_state = qpos_data[:-1]  # (T-1, 14)
            ep_action = qpos_data[1:]  # (T-1, 14) ä¸‹ä¸€ä¸ªçŠ¶æ€ä½œä¸ºaction
            ep_point_cloud = point_clouds[:-1]  # (T-1, 1024, 6)
            ep_images = images[:-1]  # (T-1, 4, 240, 320, 3)
            ep_keyframe_mask = keyframe_mask[:-1]  # (T-1,)
            ep_left_endpose = eef_data[:-1, :7]  # (T-1, 7) å·¦è‡‚å·²ç»åœ¨å·¦è‡‚åŸºåº§ç³»
            # å³è‡‚: å…ˆè½¬åˆ°Headç³»,å†è½¬åˆ°LeftBaseç³» (å’Œç‚¹äº‘å˜æ¢ä¸€è‡´)
            ep_right_endpose = transform_right_endpose_to_left_base(eef_data[:-1, 7:14], T_H_LB, T_H_RB)
            
            # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–Zarræ•°æ®é›†
            if not zarr_datasets:
                print("\nğŸ“¦ åˆå§‹åŒ–Zarræ•°æ®é›†...")
                chunks = {
                    "state": (100, 14),
                    "action": (100, 14),
                    "point_cloud": (100, FPS_SAMPLE_POINTS, 6),
                    "images": (100, 4, 240, 320, 3),
                    "keyframe_mask": (100,),
                    "left_endpose": (100, 7),
                    "right_endpose": (100, 7),
                    "episode_ends": (100,)
                }
                
                zarr_datasets["state"] = zarr_data.create_dataset(
                    "state", shape=(0, 14), maxshape=(None, 14), 
                    chunks=chunks["state"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["action"] = zarr_data.create_dataset(
                    "action", shape=(0, 14), maxshape=(None, 14),
                    chunks=chunks["action"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["point_cloud"] = zarr_data.create_dataset(
                    "point_cloud", shape=(0, FPS_SAMPLE_POINTS, 6), maxshape=(None, FPS_SAMPLE_POINTS, 6),
                    chunks=chunks["point_cloud"], dtype=np.float32, compressor=compressor
                )
                zarr_datasets["images"] = zarr_data.create_dataset(
                    "images", shape=(0, 4, 240, 320, 3), maxshape=(None, 4, 240, 320, 3),
                    chunks=chunks["images"], dtype=np.uint8, compressor=compressor
                )
                zarr_datasets["keyframe_mask"] = zarr_data.create_dataset(
                    "keyframe_mask", shape=(0,), maxshape=(None,),
                    chunks=chunks["keyframe_mask"], dtype=bool, compressor=compressor
                )
                zarr_datasets["left_endpose"] = zarr_data.create_dataset(
                    "left_endpose", shape=(0, 7), maxshape=(None, 7),
                    chunks=chunks["left_endpose"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["right_endpose"] = zarr_data.create_dataset(
                    "right_endpose", shape=(0, 7), maxshape=(None, 7),
                    chunks=chunks["right_endpose"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["episode_ends"] = zarr_meta.create_dataset(
                    "episode_ends", shape=(0,), maxshape=(None,),
                    chunks=chunks["episode_ends"], dtype=np.int64, compressor=compressor
                )
            
            # è¿½åŠ æ•°æ®åˆ°Zarr
            zarr_datasets["state"].append(ep_state)
            zarr_datasets["action"].append(ep_action)
            zarr_datasets["point_cloud"].append(ep_point_cloud)
            zarr_datasets["images"].append(ep_images)
            zarr_datasets["keyframe_mask"].append(ep_keyframe_mask)
            zarr_datasets["left_endpose"].append(ep_left_endpose)
            zarr_datasets["right_endpose"].append(ep_right_endpose)
            
            total_count += len(ep_state)
            zarr_datasets["episode_ends"].append([total_count])
            
        except Exception as e:
            print(f"\nâŒ {hdf5_filename} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nâœ… ä»»åŠ¡ {task_name} è½¬æ¢å®Œæˆ!")
    print(f"   æ€»å¸§æ•°: {total_count}")
    print(f"   Episodes: {len(zarr_datasets['episode_ends'][:])}")
    print(f"   ä¿å­˜è·¯å¾„: {save_dir}")
    
    # æ‰“å°ç»Ÿè®¡
    keyframe_count = np.sum(zarr_datasets["keyframe_mask"][:])
    print(f"   å…³é”®å¸§æ•°: {keyframe_count} ({keyframe_count/total_count*100:.2f}%)")
    print(f"{'='*80}\n")


def convert_all_tasks(max_episodes=None, task_filter=None):
    """
    è½¬æ¢datasetsç›®å½•ä¸‹æ‰€æœ‰ä»»åŠ¡
    
    Args:
        max_episodes: æ¯ä¸ªä»»åŠ¡æœ€å¤šè½¬æ¢å¤šå°‘ä¸ªepisode (Noneè¡¨ç¤ºå…¨éƒ¨)
        task_filter: ä»»åŠ¡åç§°è¿‡æ»¤å™¨ (Noneè¡¨ç¤ºå…¨éƒ¨ä»»åŠ¡, æˆ–æŒ‡å®šä»»åŠ¡ååˆ—è¡¨)
    """
    print("\n" + "="*80)
    print("ğŸš€ HDF5 to Zarr æ‰¹é‡è½¬æ¢å·¥å…·")
    print("="*80)
    
    # æ‰«ædatasetsç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
    if not os.path.exists(DATASETS_DIR):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATASETS_DIR}")
        return
    
    # è·å–æ‰€æœ‰åŒ…å«HDF5æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹
    task_dirs = []
    for item in os.listdir(DATASETS_DIR):
        item_path = os.path.join(DATASETS_DIR, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«HDF5æ–‡ä»¶
            hdf5_files = [f for f in os.listdir(item_path) if f.endswith('.hdf5')]
            if len(hdf5_files) > 0:
                task_dirs.append((item, item_path))
    
    if len(task_dirs) == 0:
        print(f"âŒ åœ¨ {DATASETS_DIR} ä¸‹æœªæ‰¾åˆ°åŒ…å«HDF5æ–‡ä»¶çš„ä»»åŠ¡æ–‡ä»¶å¤¹")
        return
    
    # åº”ç”¨è¿‡æ»¤å™¨
    if task_filter is not None:
        if isinstance(task_filter, str):
            task_filter = [task_filter]
        task_dirs = [(name, path) for name, path in task_dirs if name in task_filter]
        
        if len(task_dirs) == 0:
            print(f"âŒ æ²¡æœ‰åŒ¹é…çš„ä»»åŠ¡: {task_filter}")
            return
    
    print(f"\nğŸ“‹ å‘ç° {len(task_dirs)} ä¸ªä»»åŠ¡:")
    for i, (task_name, _) in enumerate(task_dirs, 1):
        print(f"   {i}. {task_name}")
    
    print(f"\nğŸ’¾ è¾“å‡ºç›®å½•: {DATASETS_ZARR_DIR}")
    
    # é€ä¸ªè½¬æ¢ä»»åŠ¡
    success_count = 0
    failed_tasks = []
    
    for task_name, task_path in task_dirs:
        try:
            convert_task_to_zarr(task_name, task_path, max_episodes)
            success_count += 1
        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task_name} è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed_tasks.append(task_name)
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š è½¬æ¢æ€»ç»“")
    print("="*80)
    print(f"âœ… æˆåŠŸ: {success_count}/{len(task_dirs)} ä¸ªä»»åŠ¡")
    if failed_tasks:
        print(f"âŒ å¤±è´¥çš„ä»»åŠ¡: {', '.join(failed_tasks)}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {DATASETS_ZARR_DIR}")
    print("="*80 + "\n")

# ================= ä¸»ç¨‹åº =================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å°†HDF5æ•°æ®é›†è½¬æ¢ä¸ºZarræ ¼å¼ (å«ç‚¹äº‘ç”Ÿæˆ)")
    parser.add_argument("--max_episodes", type=int, default=1, help="æ¯ä¸ªä»»åŠ¡æœ€å¤šè½¬æ¢å¤šå°‘ä¸ªepisodes (Noneè¡¨ç¤ºå…¨éƒ¨)")
    parser.add_argument("--task", type=str, default=None, help="æŒ‡å®šè¦è½¬æ¢çš„ä»»åŠ¡åç§° (é»˜è®¤è½¬æ¢æ‰€æœ‰ä»»åŠ¡)")
    
    args = parser.parse_args()
    
    convert_all_tasks(
        max_episodes=args.max_episodes,
        task_filter=args.task
    )
