#!/usr/bin/env python3
# -*- coding:utf-8 -*-

"""
å°†HDF5æ•°æ®é›†è½¬æ¢ä¸ºZarræ ¼å¼,åŒ…å«ç‚¹äº‘ç”Ÿæˆ
ç”¨æ³•: python convert_hdf5_to_zarr.py --num_episodes 100 --max_episodes 5 (debugæ¨¡å¼)
"""

import os
import h5py
import zarr
import numpy as np
import cv2
import json
import argparse
import shutil
from tqdm import tqdm
from scipy.spatial.transform import Rotation as R
import open3d as o3d

# ================= é…ç½® =================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(BASE_DIR)  # ä¸Šä¸€çº§ç›®å½•
DATASETS_DIR = os.path.join(BASE_DIR, "datasets")
DATASETS_ZARR_DIR = os.path.join(BASE_DIR, "datasets_zarr")
CALIBRATION_DIR = os.path.join(BASE_DIR, "calibration_results")
INTRINSICS_FILE = os.path.join(CALIBRATION_DIR, "D405_intrinsics.json")

# ç‚¹äº‘é…ç½®
MAX_DEPTH_Head = 1.0  # ç±³
MAX_DEPTH_Hand = 0.6  # ç±³
FPS_SAMPLE_POINTS = 1024  # ç‚¹äº‘é‡‡æ ·ç‚¹æ•°

# å·¥ä½œç©ºé—´è£å‰ª (ç›¸å¯¹äºå·¦è‡‚åŸºåº§åæ ‡ç³»)
USE_WORKSPACE_CROP = True
WORKSPACE_X_RANGE = [-0.4, 0.5]
WORKSPACE_Y_RANGE = [-0.5, 3.0]
WORKSPACE_Z_RANGE = [-0.2, 1.0]

# å…³é”®å¸§æ£€æµ‹
GRIPPER_DELTA = 0.05  # å¤¹çˆªå˜åŒ–é˜ˆå€¼
MIN_INTERVAL = 20  # æœ€å°å…³é”®å¸§é—´éš”

# ================= æ ‡å®šåŠ è½½å‡½æ•° =================

def load_intrinsics(camera_name):
    """åŠ è½½ç›¸æœºå†…å‚"""
    with open(INTRINSICS_FILE, 'r') as f:
        all_data = json.load(f)
    d = all_data[camera_name]
    return d['fx'], d['fy'], d['cx'], d['cy']

def load_calibration_matrix(filename):
    """åŠ è½½æ ‡å®šçŸ©é˜µ"""
    path = os.path.join(CALIBRATION_DIR, filename)
    if os.path.exists(path):
        if path.endswith('.npy'):
            return np.load(path)
        elif path.endswith('.txt'):
            return np.loadtxt(path)
    print(f"âŒ ç¼ºå°‘æ ‡å®šæ–‡ä»¶: {filename}")
    return np.eye(4)

# ================= ç‚¹äº‘ç”Ÿæˆå‡½æ•° =================

def eef_to_matrix(eef_pose):
    """å°†end-effector poseè½¬æ¢ä¸º4x4å˜æ¢çŸ©é˜µ"""
    if eef_pose is None or len(eef_pose) < 6:
        return np.eye(4)
    t = np.array(eef_pose[:3])
    r = R.from_euler('xyz', eef_pose[3:6]).as_matrix()
    T = np.eye(4)
    T[:3, :3] = r
    T[:3, 3] = t
    return T

def depth_to_point_cloud(depth_img, color_img, fx, fy, cx, cy, max_depth=None):
    """å°†æ·±åº¦å›¾å’Œå½©è‰²å›¾è½¬æ¢ä¸ºç‚¹äº‘"""
    h, w = depth_img.shape
    u, v = np.meshgrid(np.arange(w), np.arange(h))
    
    valid = depth_img > 0
    if max_depth is not None:
        valid = valid & (depth_img < max_depth * 1000)
    
    z = depth_img[valid].astype(np.float32) / 1000.0
    u = u[valid]
    v = v[valid]
    
    x = (u - cx) * z / fx
    y = (v - cy) * z / fy
    
    b = color_img[valid, 0].astype(np.float32) / 255.0
    g = color_img[valid, 1].astype(np.float32) / 255.0
    r = color_img[valid, 2].astype(np.float32) / 255.0
    
    xyz = np.stack((x, y, z), axis=1)
    rgb = np.stack((r, g, b), axis=1)
    
    return np.hstack((xyz, rgb))

def transform_point_cloud(cloud, T):
    """å˜æ¢ç‚¹äº‘"""
    xyz = cloud[:, :3]
    rgb = cloud[:, 3:]
    
    ones = np.ones((xyz.shape[0], 1))
    xyz_homo = np.hstack((xyz, ones))
    xyz_trans = (T @ xyz_homo.T).T
    
    return np.hstack((xyz_trans[:, :3], rgb))

def crop_point_cloud(cloud_np, x_range, y_range, z_range):
    """è£å‰ªç‚¹äº‘"""
    xyz = cloud_np[:, :3]
    mask = (
        (xyz[:, 0] >= x_range[0]) & (xyz[:, 0] <= x_range[1]) &
        (xyz[:, 1] >= y_range[0]) & (xyz[:, 1] <= y_range[1]) &
        (xyz[:, 2] >= z_range[0]) & (xyz[:, 2] <= z_range[1])
    )
    return cloud_np[mask]

def numpy_to_o3d(cloud_np):
    """è½¬æ¢numpyæ•°ç»„åˆ°open3dç‚¹äº‘"""
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(cloud_np[:, :3])
    pcd.colors = o3d.utility.Vector3dVector(cloud_np[:, 3:])
    return pcd

def generate_point_cloud_single_frame(head_depth, head_color, left_depth, left_color, 
                                     right_depth, right_color, left_eef, right_eef,
                                     intrinsics, T_H_LB, T_H_RB, T_LE_LC, T_RE_RC,
                                     T_LB_H):
    """
    ç”Ÿæˆå•å¸§ç‚¹äº‘ (åœ¨å·¦è‡‚åŸºåº§åæ ‡ç³»ä¸‹)
    è¿”å›: (N, 6) numpy array, N <= FPS_SAMPLE_POINTS
    """
    clouds_global = []
    
    # 1. Head Camera
    fx, fy, cx, cy = intrinsics['head']
    pc_head = depth_to_point_cloud(head_depth, head_color, fx, fy, cx, cy, max_depth=MAX_DEPTH_Head)
    if len(pc_head) > 0:
        clouds_global.append(pc_head)
    
    # 2. Left Wrist Camera
    fx, fy, cx, cy = intrinsics['left']
    pc_left = depth_to_point_cloud(left_depth, left_color, fx, fy, cx, cy, max_depth=MAX_DEPTH_Hand)
    if len(pc_left) > 0:
        T_LB_LE = eef_to_matrix(left_eef)
        T_total_left = T_H_LB @ T_LB_LE @ T_LE_LC
        pc_left_global = transform_point_cloud(pc_left, T_total_left)
        clouds_global.append(pc_left_global)
    
    # 3. Right Wrist Camera
    fx, fy, cx, cy = intrinsics['right']
    pc_right = depth_to_point_cloud(right_depth, right_color, fx, fy, cx, cy, max_depth=MAX_DEPTH_Hand)
    if len(pc_right) > 0:
        T_RB_RE = eef_to_matrix(right_eef)
        # å˜æ¢è·¯å¾„: Cam -> End -> Base -> HeadCam
        # P_Head = T_H_RB @ T_RB_RE @ T_RE_RC @ P_Cam
        T_total_right = T_H_RB @ T_RB_RE @ T_RE_RC
        pc_right_global = transform_point_cloud(pc_right, T_total_right)
        clouds_global.append(pc_right_global)
    
    if len(clouds_global) == 0:
        # è¿”å›ç©ºç‚¹äº‘
        return np.zeros((FPS_SAMPLE_POINTS, 6), dtype=np.float32)
    
    # 4. åˆå¹¶å¹¶è½¬æ¢åˆ°å·¦è‡‚åŸºåº§åæ ‡ç³»
    # æ³¨æ„: T_LB_Hå®é™…å°±æ˜¯Head->LeftBase, å’Œpointcloud_from_hdf5.pyä¸­ç”¨æ³•ä¸€è‡´
    merged_cloud = np.vstack(clouds_global)
    merged_cloud = transform_point_cloud(merged_cloud, T_LB_H)
    
    # 5. å·¥ä½œç©ºé—´è£å‰ª
    if USE_WORKSPACE_CROP:
        merged_cloud = crop_point_cloud(merged_cloud, WORKSPACE_X_RANGE, 
                                       WORKSPACE_Y_RANGE, WORKSPACE_Z_RANGE)
    
    if len(merged_cloud) == 0:
        return np.zeros((FPS_SAMPLE_POINTS, 6), dtype=np.float32)
    
    # 6. ä¸‹é‡‡æ ·
    pcd = numpy_to_o3d(merged_cloud)
    
    # å»å™ª
    pcd_clean, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
    
    # ä½“ç´ ä¸‹é‡‡æ ·
    pcd_voxel = pcd_clean.voxel_down_sample(voxel_size=0.005)
    
    # FPSé‡‡æ ·
    if len(pcd_voxel.points) > FPS_SAMPLE_POINTS:
        pcd_fps = pcd_voxel.farthest_point_down_sample(FPS_SAMPLE_POINTS)
    else:
        pcd_fps = pcd_voxel
    
    # è½¬æ¢å›numpy
    pts = np.asarray(pcd_fps.points)
    clrs = np.asarray(pcd_fps.colors)
    result = np.hstack((pts, clrs)).astype(np.float32)
    
    # Padåˆ°å›ºå®šå¤§å°
    if len(result) < FPS_SAMPLE_POINTS:
        padding = np.zeros((FPS_SAMPLE_POINTS - len(result), 6), dtype=np.float32)
        result = np.vstack((result, padding))
    
    return result

# ================= HDF5æ•°æ®è¯»å– =================

def decode_jpeg(data):
    """è§£ç JPEGæ•°æ®"""
    img_bgr = cv2.imdecode(np.frombuffer(data, dtype=np.uint8), cv2.IMREAD_COLOR)
    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)  # è½¬æ¢ä¸ºRGB

def load_hdf5_episode(hdf5_path):
    """
    è¯»å–å•ä¸ªHDF5æ–‡ä»¶
    è¿”å›: dict with keys: eef, images (head, left_wrist, right_wrist), depths, qpos
    """
    with h5py.File(hdf5_path, 'r') as f:
        # è¯»å–end-effectoræ•°æ®
        eef_data = f['observations/eef'][()]  # (T, 14)
        qpos_data = f['observations/qpos'][()]  # (T, 14)
        
        # è¯»å–å›¾åƒ (JPEGç¼–ç )
        head_imgs = [decode_jpeg(d) for d in f['observations/images/head'][()]]
        left_imgs = [decode_jpeg(d) for d in f['observations/images/left_wrist'][()]]
        right_imgs = [decode_jpeg(d) for d in f['observations/images/right_wrist'][()]]
        
        # è¯»å–æ·±åº¦
        head_depths = f['observations/images_depth/head'][()]
        left_depths = f['observations/images_depth/left_wrist'][()]
        right_depths = f['observations/images_depth/right_wrist'][()]
        
        return {
            'eef': eef_data,
            'qpos': qpos_data,
            'head_images': np.array(head_imgs),
            'left_images': np.array(left_imgs),
            'right_images': np.array(right_imgs),
            'head_depths': head_depths,
            'left_depths': left_depths,
            'right_depths': right_depths,
        }

# ================= å…³é”®å¸§æ£€æµ‹ =================

def transform_right_endpose_to_left_base(right_eef_array, T_H_LB, T_H_RB):
    """
    å°†å³è‡‚æœ«ç«¯å§¿æ€ä»å³è‡‚åŸºåº§åæ ‡ç³»è½¬æ¢åˆ°å·¦è‡‚åŸºåº§åæ ‡ç³»
    right_eef_array: (N, 7) [x, y, z, rx, ry, rz, gripper]
    T_H_LB: (4, 4) Headåˆ°å·¦è‡‚åŸºåº§çš„å˜æ¢çŸ©é˜µ (æ³¨æ„: æ–‡ä»¶åhead_base_to_leftçš„å«ä¹‰)
    T_H_RB: (4, 4) Headåˆ°å³è‡‚åŸºåº§çš„å˜æ¢çŸ©é˜µ
    è¿”å›: (N, 7) åœ¨å·¦è‡‚åŸºåº§åæ ‡ç³»ä¸‹çš„å§¿æ€
    
    å˜æ¢é“¾: Head -> RightBase -> RightEnd, ç„¶åè½¬åˆ°LeftBase
    å³: T_LB_RE = T_H_LB @ T_H_RB @ T_RB_RE
    """
    N = len(right_eef_array)
    result = np.zeros_like(right_eef_array)
    
    for i in range(N):
        # æå–å³è‡‚æœ«ç«¯åœ¨å³è‡‚åŸºåº§ç³»ä¸‹çš„å§¿æ€
        T_RB_RE = eef_to_matrix(right_eef_array[i])
        
        # è½¬æ¢åˆ°å·¦è‡‚åŸºåº§ç³»: Head -> RightBase -> RightEnd, å†è½¬åˆ°LeftBase
        T_LB_RE = T_H_LB @ T_H_RB @ T_RB_RE
        
        # æå–ä½ç½®
        result[i, :3] = T_LB_RE[:3, 3]
        
        # æå–æ—‹è½¬(è½¬æ¢ä¸ºæ¬§æ‹‰è§’)
        rot_matrix = T_LB_RE[:3, :3]
        result[i, 3:6] = R.from_matrix(rot_matrix).as_euler('xyz')
        
        # å¤¹çˆªå€¼ä¸å˜
        result[i, 6] = right_eef_array[i, 6]
    
    return result

def get_keyframe_mask(eef_data, gripper_delta=0.05, min_interval=5):
    """
    ç”Ÿæˆå…³é”®å¸§mask (åªåŸºäºå¤¹çˆªå¼€åˆ,ä¸è€ƒè™‘æš‚åœ)
    eef_data: (T, 14) [left(7), right(7)]
    """
    T = len(eef_data)
    mask = np.zeros(T, dtype=bool)
    
    # æå–å¤¹çˆªçŠ¶æ€
    left_gripper = eef_data[:, 6]  # ç¬¬7ç»´
    right_gripper = eef_data[:, 13]  # ç¬¬14ç»´
    
    # è®¡ç®—å¤¹çˆªå˜åŒ–
    left_diff = np.abs(np.diff(left_gripper, prepend=left_gripper[0]))
    right_diff = np.abs(np.diff(right_gripper, prepend=right_gripper[0]))
    
    # ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§æ€»æ˜¯å…³é”®å¸§
    mask[0] = True
    mask[-1] = True
    
    last_keyframe_idx = 0
    for i in range(1, T - 1):
        # æ£€æŸ¥å¤¹çˆªæ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
        is_gripper_change = (left_diff[i] > gripper_delta) or (right_diff[i] > gripper_delta)
        
        # å¼ºåˆ¶æœ€å°é—´éš”
        if (i - last_keyframe_idx) > min_interval and is_gripper_change:
            mask[i] = True
            last_keyframe_idx = i
    
    return mask

# ================= ä¸»è½¬æ¢å‡½æ•° =================

def convert_task_to_zarr(task_name, task_dir, max_episodes=None):
    """
    å°†å•ä¸ªä»»åŠ¡çš„HDF5æ•°æ®è½¬æ¢ä¸ºZarræ ¼å¼
    
    Args:
        task_name: ä»»åŠ¡åç§° (æ–‡ä»¶å¤¹å)
        task_dir: ä»»åŠ¡æ–‡ä»¶å¤¹è·¯å¾„
        max_episodes: ç”¨äºdebug,åªè½¬æ¢å‰Nä¸ªepisode (Noneè¡¨ç¤ºè½¬æ¢å…¨éƒ¨)
    """
    # è‡ªåŠ¨æ‰«æHDF5æ–‡ä»¶
    print(f"\n{'='*80}")
    print(f"ğŸ¯ ä»»åŠ¡: {task_name}")
    print(f"{'='*80}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {task_dir}")
    
    hdf5_files = sorted([f for f in os.listdir(task_dir) if f.endswith('.hdf5')])
    print(f"ğŸ” æ‰¾åˆ° {len(hdf5_files)} ä¸ªHDF5æ–‡ä»¶")
    
    if len(hdf5_files) == 0:
        print(f"âš ï¸  ä»»åŠ¡ {task_name} æ²¡æœ‰HDF5æ–‡ä»¶,è·³è¿‡")
        return
    
    # è¾“å‡ºè·¯å¾„
    os.makedirs(DATASETS_ZARR_DIR, exist_ok=True)
    save_dir = os.path.join(DATASETS_ZARR_DIR, f"{task_name}.zarr")
    
    if os.path.exists(save_dir):
        print(f"âš ï¸  åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {save_dir}")
        shutil.rmtree(save_dir)
    
    # åˆ›å»ºZarræ ¹
    zarr_root = zarr.group(save_dir)
    zarr_data = zarr_root.create_group("data")
    zarr_meta = zarr_root.create_group("meta")
    
    # åŠ è½½æ ‡å®šæ•°æ®
    print("\nğŸ“ åŠ è½½æ ‡å®šæ–‡ä»¶...")
    T_LE_LC = load_calibration_matrix("left_eye_in_hand.npy")
    T_RE_RC = load_calibration_matrix("right_eye_in_hand.npy")
    T_LB_H = load_calibration_matrix("head_base_to_left_refined_icp.txt")
    T_RB_H = load_calibration_matrix("head_base_to_right_refined_icp.txt")
    
    if np.array_equal(T_LB_H, np.eye(4)):
        T_LB_H = load_calibration_matrix("head_base_to_left.npy")
    if np.array_equal(T_RB_H, np.eye(4)):
        T_RB_H = load_calibration_matrix("head_base_to_right.npy")
    
    # æ³¨æ„: æ–‡ä»¶åhead_base_to_leftå®é™…è¡¨ç¤º Head->LeftBase çš„å˜æ¢
    # æ–‡ä»¶åhead_base_to_rightå®é™…è¡¨ç¤º RightBase->Head çš„å˜æ¢ (éœ€è¦å–é€†å¾—åˆ°Head->RightBase)
    # å’Œpointcloud_from_hdf5.pyä¿æŒä¸€è‡´
    T_H_LB = T_LB_H
    T_H_RB = np.linalg.inv(T_RB_H)
    
    intrinsics = {
        'head': load_intrinsics('head'),
        'left': load_intrinsics('left'),
        'right': load_intrinsics('right')
    }
    print("âœ… æ ‡å®šæ–‡ä»¶åŠ è½½å®Œæˆ")
    
    # åˆå§‹åŒ–Zarræ•°æ®é›†
    compressor = zarr.Blosc(cname="zstd", clevel=3, shuffle=1)
    zarr_datasets = {}
    
    total_count = 0
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    files_to_process = hdf5_files[:max_episodes] if max_episodes is not None else hdf5_files
    
    print(f"\nğŸ”„ å¼€å§‹è½¬æ¢ (å…± {len(files_to_process)} episodes)...")
    print(f"ç¬¬ä¸€ä¸ªæ–‡ä»¶: {files_to_process[0]}")
    if len(files_to_process) > 1:
        print(f"æœ€åä¸€ä¸ªæ–‡ä»¶: {files_to_process[-1]}")
    
    for hdf5_filename in tqdm(files_to_process, desc=f"Converting {task_name}"):
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        hdf5_path = os.path.join(task_dir, hdf5_filename)
        
        if not os.path.exists(hdf5_path):
            print(f"\nâš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {hdf5_path}")
            continue
        
        try:
            # è¯»å–HDF5æ•°æ®
            data = load_hdf5_episode(hdf5_path)
            eef_data = data['eef']
            qpos_data = data['qpos']
            
            T = len(eef_data)
            if T < 2:
                print(f"\nâš ï¸  {hdf5_filename} å¤ªçŸ­,è·³è¿‡")
                continue
            
            # åˆ†ç¦»å·¦å³è‡‚
            left_eef = eef_data[:, :7]
            right_eef = eef_data[:, 7:14]
            
            # ç”Ÿæˆç‚¹äº‘ (æ¯ä¸€å¸§) - æ˜¾ç¤ºå¸§çº§åˆ«è¿›åº¦
            point_clouds = []
            print(f"\n  ğŸ“Š {hdf5_filename}: ç”Ÿæˆ {T} å¸§ç‚¹äº‘...")
            for t in tqdm(range(T), desc=f"  Processing frames", leave=False, ncols=80):
                pc = generate_point_cloud_single_frame(
                    data['head_depths'][t], data['head_images'][t],
                    data['left_depths'][t], data['left_images'][t],
                    data['right_depths'][t], data['right_images'][t],
                    left_eef[t], right_eef[t],
                    intrinsics, T_H_LB, T_H_RB, T_LE_LC, T_RE_RC, T_LB_H
                )
                point_clouds.append(pc)
            
            point_clouds = np.array(point_clouds)  # (T, 1024, 6)
            
            # ç»„ç»‡å›¾åƒ (4ä¸ªç›¸æœº: head, left, right, è¿˜éœ€è¦ä¸€ä¸ªfront?)
            # æ ¹æ®ç›®æ ‡æ ¼å¼: (T, 4, 240, 320, 3)
            # å‡è®¾æˆ‘ä»¬resizeåˆ°240x320 (å›¾åƒå·²ç»æ˜¯RGBæ ¼å¼)
            def resize_images(imgs):
                return np.array([cv2.resize(img, (320, 240)) for img in imgs])
            
            head_resized = resize_images(data['head_images'])
            left_resized = resize_images(data['left_images'])
            right_resized = resize_images(data['right_images'])
            
            # åˆ›å»º4ä¸ªç›¸æœºçš„å›¾åƒ (å¦‚æœåªæœ‰3ä¸ª,å¤åˆ¶ä¸€ä¸ª)
            images = np.stack([head_resized, head_resized, left_resized, right_resized], axis=1)  # (T, 4, 240, 320, 3)
            
            # è®¡ç®—å…³é”®å¸§mask
            keyframe_mask = get_keyframe_mask(eef_data, GRIPPER_DELTA, MIN_INTERVAL)
            
            # å‡†å¤‡episodeæ•°æ® (state[t] + action[t] -> state[t+1])
            ep_state = qpos_data[:-1]  # (T-1, 14)
            ep_action = qpos_data[1:]  # (T-1, 14) ä¸‹ä¸€ä¸ªçŠ¶æ€ä½œä¸ºaction
            ep_point_cloud = point_clouds[:-1]  # (T-1, 1024, 6)
            ep_images = images[:-1]  # (T-1, 4, 240, 320, 3)
            ep_keyframe_mask = keyframe_mask[:-1]  # (T-1,)
            ep_left_endpose = eef_data[:-1, :7]  # (T-1, 7) å·¦è‡‚å·²ç»åœ¨å·¦è‡‚åŸºåº§ç³»
            # å³è‡‚: å…ˆè½¬åˆ°Headç³»,å†è½¬åˆ°LeftBaseç³» (å’Œç‚¹äº‘å˜æ¢ä¸€è‡´)
            ep_right_endpose = transform_right_endpose_to_left_base(eef_data[:-1, 7:14], T_H_LB, T_H_RB)
            
            # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–Zarræ•°æ®é›†
            if not zarr_datasets:
                print("\nğŸ“¦ åˆå§‹åŒ–Zarræ•°æ®é›†...")
                chunks = {
                    "state": (100, 14),
                    "action": (100, 14),
                    "point_cloud": (100, FPS_SAMPLE_POINTS, 6),
                    "images": (100, 4, 240, 320, 3),
                    "keyframe_mask": (100,),
                    "left_endpose": (100, 7),
                    "right_endpose": (100, 7),
                    "episode_ends": (100,)
                }
                
                zarr_datasets["state"] = zarr_data.create_dataset(
                    "state", shape=(0, 14), maxshape=(None, 14), 
                    chunks=chunks["state"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["action"] = zarr_data.create_dataset(
                    "action", shape=(0, 14), maxshape=(None, 14),
                    chunks=chunks["action"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["point_cloud"] = zarr_data.create_dataset(
                    "point_cloud", shape=(0, FPS_SAMPLE_POINTS, 6), maxshape=(None, FPS_SAMPLE_POINTS, 6),
                    chunks=chunks["point_cloud"], dtype=np.float32, compressor=compressor
                )
                zarr_datasets["images"] = zarr_data.create_dataset(
                    "images", shape=(0, 4, 240, 320, 3), maxshape=(None, 4, 240, 320, 3),
                    chunks=chunks["images"], dtype=np.uint8, compressor=compressor
                )
                zarr_datasets["keyframe_mask"] = zarr_data.create_dataset(
                    "keyframe_mask", shape=(0,), maxshape=(None,),
                    chunks=chunks["keyframe_mask"], dtype=bool, compressor=compressor
                )
                zarr_datasets["left_endpose"] = zarr_data.create_dataset(
                    "left_endpose", shape=(0, 7), maxshape=(None, 7),
                    chunks=chunks["left_endpose"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["right_endpose"] = zarr_data.create_dataset(
                    "right_endpose", shape=(0, 7), maxshape=(None, 7),
                    chunks=chunks["right_endpose"], dtype=np.float64, compressor=compressor
                )
                zarr_datasets["episode_ends"] = zarr_meta.create_dataset(
                    "episode_ends", shape=(0,), maxshape=(None,),
                    chunks=chunks["episode_ends"], dtype=np.int64, compressor=compressor
                )
            
            # è¿½åŠ æ•°æ®åˆ°Zarr
            zarr_datasets["state"].append(ep_state)
            zarr_datasets["action"].append(ep_action)
            zarr_datasets["point_cloud"].append(ep_point_cloud)
            zarr_datasets["images"].append(ep_images)
            zarr_datasets["keyframe_mask"].append(ep_keyframe_mask)
            zarr_datasets["left_endpose"].append(ep_left_endpose)
            zarr_datasets["right_endpose"].append(ep_right_endpose)
            
            total_count += len(ep_state)
            zarr_datasets["episode_ends"].append([total_count])
            
        except Exception as e:
            print(f"\nâŒ {hdf5_filename} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nâœ… ä»»åŠ¡ {task_name} è½¬æ¢å®Œæˆ!")
    print(f"   æ€»å¸§æ•°: {total_count}")
    print(f"   Episodes: {len(zarr_datasets['episode_ends'][:])}")
    print(f"   ä¿å­˜è·¯å¾„: {save_dir}")
    
    # æ‰“å°ç»Ÿè®¡
    keyframe_count = np.sum(zarr_datasets["keyframe_mask"][:])
    print(f"   å…³é”®å¸§æ•°: {keyframe_count} ({keyframe_count/total_count*100:.2f}%)")
    print(f"{'='*80}\n")


def convert_all_tasks(max_episodes=None, task_filter=None):
    """
    è½¬æ¢datasetsç›®å½•ä¸‹æ‰€æœ‰ä»»åŠ¡
    
    Args:
        max_episodes: æ¯ä¸ªä»»åŠ¡æœ€å¤šè½¬æ¢å¤šå°‘ä¸ªepisode (Noneè¡¨ç¤ºå…¨éƒ¨)
        task_filter: ä»»åŠ¡åç§°è¿‡æ»¤å™¨ (Noneè¡¨ç¤ºå…¨éƒ¨ä»»åŠ¡, æˆ–æŒ‡å®šä»»åŠ¡ååˆ—è¡¨)
    """
    print("\n" + "="*80)
    print("ğŸš€ HDF5 to Zarr æ‰¹é‡è½¬æ¢å·¥å…·")
    print("="*80)
    
    # æ‰«ædatasetsç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
    if not os.path.exists(DATASETS_DIR):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATASETS_DIR}")
        return
    
    # è·å–æ‰€æœ‰åŒ…å«HDF5æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹
    task_dirs = []
    for item in os.listdir(DATASETS_DIR):
        item_path = os.path.join(DATASETS_DIR, item)
        if os.path.isdir(item_path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«HDF5æ–‡ä»¶
            hdf5_files = [f for f in os.listdir(item_path) if f.endswith('.hdf5')]
            if len(hdf5_files) > 0:
                task_dirs.append((item, item_path))
    
    if len(task_dirs) == 0:
        print(f"âŒ åœ¨ {DATASETS_DIR} ä¸‹æœªæ‰¾åˆ°åŒ…å«HDF5æ–‡ä»¶çš„ä»»åŠ¡æ–‡ä»¶å¤¹")
        return
    
    # åº”ç”¨è¿‡æ»¤å™¨
    if task_filter is not None:
        if isinstance(task_filter, str):
            task_filter = [task_filter]
        task_dirs = [(name, path) for name, path in task_dirs if name in task_filter]
        
        if len(task_dirs) == 0:
            print(f"âŒ æ²¡æœ‰åŒ¹é…çš„ä»»åŠ¡: {task_filter}")
            return
    
    print(f"\nğŸ“‹ å‘ç° {len(task_dirs)} ä¸ªä»»åŠ¡:")
    for i, (task_name, _) in enumerate(task_dirs, 1):
        print(f"   {i}. {task_name}")
    
    print(f"\nğŸ’¾ è¾“å‡ºç›®å½•: {DATASETS_ZARR_DIR}")
    
    # é€ä¸ªè½¬æ¢ä»»åŠ¡
    success_count = 0
    failed_tasks = []
    
    for task_name, task_path in task_dirs:
        try:
            convert_task_to_zarr(task_name, task_path, max_episodes)
            success_count += 1
        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task_name} è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed_tasks.append(task_name)
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š è½¬æ¢æ€»ç»“")
    print("="*80)
    print(f"âœ… æˆåŠŸ: {success_count}/{len(task_dirs)} ä¸ªä»»åŠ¡")
    if failed_tasks:
        print(f"âŒ å¤±è´¥çš„ä»»åŠ¡: {', '.join(failed_tasks)}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {DATASETS_ZARR_DIR}")
    print("="*80 + "\n")

# ================= ä¸»ç¨‹åº =================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å°†HDF5æ•°æ®é›†è½¬æ¢ä¸ºZarræ ¼å¼ (å«ç‚¹äº‘ç”Ÿæˆ)")
    parser.add_argument("--max_episodes", type=int, default=None, help="æ¯ä¸ªä»»åŠ¡æœ€å¤šè½¬æ¢å¤šå°‘ä¸ªepisodes (Noneè¡¨ç¤ºå…¨éƒ¨)")
    parser.add_argument("--task", type=str, default=None, help="æŒ‡å®šè¦è½¬æ¢çš„ä»»åŠ¡åç§° (é»˜è®¤è½¬æ¢æ‰€æœ‰ä»»åŠ¡)")
    
    args = parser.parse_args()
    
    convert_all_tasks(
        max_episodes=args.max_episodes,
        task_filter=args.task
    )
