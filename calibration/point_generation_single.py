#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import os
import cv2
import json
import numpy as np
import glob
from scipy.spatial.transform import Rotation as R
import open3d as o3d

# ================= é…ç½® =================
DATA_ROOT = "data_points_test"
RESULTS_DIR = "calibration_results"
INTRINSICS_FILE = "calibration_data_ark/intrinsics.json"
OUTPUT_DIR = "point_cloud_output"
USE_ICP_REFINEMENT = True # è®¾ç½®ä¸º True å¯ç”¨ ICP ä¿®æ­£

# ================= è¾…åŠ©å‡½æ•° =================

def load_intrinsics(camera_name):
    with open(INTRINSICS_FILE, 'r') as f:
        all_data = json.load(f)
    d = all_data[camera_name]
    fx, fy = d['fx'], d['fy']
    cx, cy = d['cx'], d['cy']
    return fx, fy, cx, cy

def load_calibration_matrix(filename):
    path = os.path.join(RESULTS_DIR, filename)
    if os.path.exists(path):
        return np.load(path)
    print(f"âŒ ç¼ºå°‘æ ‡å®šæ–‡ä»¶: {filename}")
    return np.eye(4)

def pose_to_matrix(pose):
    # pose: [x, y, z, rx, ry, rz]
    if pose is None: return np.eye(4)
    t = np.array(pose[:3])
    r = R.from_euler('xyz', pose[3:]).as_matrix()
    T = np.eye(4)
    T[:3, :3] = r
    T[:3, 3] = t
    return T

def depth_to_point_cloud(depth_img, color_img, fx, fy, cx, cy):
    """
    è¾“å…¥:
      depth_img: (H, W) uint16, unit mm
      color_img: (H, W, 3) BGR uint8
    è¾“å‡º:
      points: (N, 6) [x, y, z, r, g, b] in meters
    """
    h, w = depth_img.shape
    # åˆ›å»ºç½‘æ ¼
    u, v = np.meshgrid(np.arange(w), np.arange(h))
    
    # è¿‡æ»¤æ— æ•ˆæ·±åº¦ (0)
    valid = (depth_img > 0) & (depth_img < 3000) # è¿‡æ»¤æ‰å¤ªè¿œçš„ç‚¹(>3m)
    
    z = depth_img[valid].astype(np.float32) / 1000.0
    u = u[valid]
    v = v[valid]
    
    x = (u - cx) * z / fx
    y = (v - cy) * z / fy
    
    # é¢œè‰² (BGR -> RGB, 0-1)
    b = color_img[valid, 0].astype(np.float32) / 255.0
    g = color_img[valid, 1].astype(np.float32) / 255.0
    r = color_img[valid, 2].astype(np.float32) / 255.0
    
    # æ‹¼æ¥ (N, 6)
    xyz = np.stack((x, y, z), axis=1)
    rgb = np.stack((r, g, b), axis=1)
    return np.hstack((xyz, rgb))

def transform_point_cloud(cloud, T):
    """cloud: (N, 6), T: (4, 4)"""
    xyz = cloud[:, :3]
    rgb = cloud[:, 3:]
    
    # é½æ¬¡å˜æ¢
    ones = np.ones((xyz.shape[0], 1))
    xyz_homo = np.hstack((xyz, ones))
    
    xyz_trans = (T @ xyz_homo.T).T
    xyz_new = xyz_trans[:, :3]
    
    return np.hstack((xyz_new, rgb))

def numpy_to_o3d(cloud_np):
    """è½¬æ¢ (N, 6) numpy æ•°ç»„ åˆ° open3d.geometry.PointCloud"""
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(cloud_np[:, :3])
    pcd.colors = o3d.utility.Vector3dVector(cloud_np[:, 3:])
    return pcd

def visualize_clouds(clouds_dict):
    """
    ä½¿ç”¨ Open3D å¯è§†åŒ–ã€‚
    clouds_dict: {"Window Name": point_cloud_numpy_array}
    """
    for name, cloud_np in clouds_dict.items():
        if cloud_np is None or len(cloud_np) == 0:
            continue
            
        print(f"ğŸ‘€ æ˜¾ç¤º: {name} (æŒ‰ 'Q' å…³é—­å½“å‰çª—å£è¿›å…¥ä¸‹ä¸€ä¸ªï¼Œæ‹–åŠ¨é¼ æ ‡æ—‹è½¬)")
        pcd = numpy_to_o3d(cloud_np)
        
        # åˆ›å»ºåæ ‡è½´æ–¹ä¾¿å‚è€ƒ
        axes = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2, origin=[0, 0, 0])
        
        o3d.visualization.draw_geometries([pcd, axes], window_name=name, width=800, height=600)

def visualize_merged(clouds_list, frames_list=[]):
    """
    æ˜¾ç¤ºåˆå¹¶åçš„ç‚¹äº‘åœºæ™¯
    frames_list: [(T_matrix, label_string), ...]
    """
    print(f"ğŸ‘€ æ˜¾ç¤º: Merged Result (æŒ‰ 'Q' å…³é—­)")
    geometries = []
    
    # 1. World Origin (Head Camera)
    axes_world = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2, origin=[0, 0, 0])
    geometries.append(axes_world)

    # 2. Point Clouds
    for cloud_obj in clouds_list:
        if isinstance(cloud_obj, np.ndarray):
            pcd = numpy_to_o3d(cloud_obj)
        else:
            pcd = cloud_obj
        geometries.append(pcd)
        
    # 3. Coordinate Frames
    for T, label in frames_list:
        # T is 4x4
        if T is not None:
            # Create a small coordinate frame
            frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15)
            frame.transform(T)
            geometries.append(frame)
            
            # Text label (Open3D doesn't support 3D text easily in simple visualizer, usually we just rely on axes)
            # Red=X, Green=Y, Blue=Z
            print(f"  -> Added Frame: {label} at pos {T[:3, 3]}")

    o3d.visualization.draw_geometries(geometries, window_name="Merged Point Cloud", width=1280, height=720)


def run_icp_refinement(source_np, target_np, title="ICP"):
    """
    è¿è¡Œ ICP é…å‡†
    è¾“å…¥: source_np (N, 6), target_np (M, 6)
    è¾“å‡º: transformation (4, 4), aligned_source (pcd)
    """
    print(f"\n[{title}] Running ICP...")
    
    # è½¬æ¢å¹¶é¢„å¤„ç†
    source = numpy_to_o3d(source_np)
    target = numpy_to_o3d(target_np)
    
    # é™é‡‡æ ·å¹¶è®¡ç®—æ³•å‘é‡ (å¯¹äº point-to-plane ICP å¾ˆé‡è¦)
    voxel_size = 0.005
    source_down = source.voxel_down_sample(voxel_size)
    target_down = target.voxel_down_sample(voxel_size)
    
    source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size*2, max_nn=30))
    target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size*2, max_nn=30))
    
    # åˆå§‹å¯¹é½å‡è®¾ä¸º Identity (å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†åˆæ­¥æ ‡å®š)
    trans_init = np.eye(4)
    threshold = 0.02 # 2cm è·ç¦»é˜ˆå€¼
    
    # ä½¿ç”¨ Point-to-Plane ICP (é€šå¸¸æ¯” Point-to-Point æ”¶æ•›æ›´å¥½)
    try:
        reg_p2l = o3d.pipelines.registration.registration_icp(
            source_down, target_down, threshold, trans_init,
            o3d.pipelines.registration.TransformationEstimationPointToPlane(),
            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000)
        )
    except Exception as e:
        print(f"ICP Error: {e}, falling back to Point-to-Point")
        reg_p2l = o3d.pipelines.registration.registration_icp(
            source_down, target_down, threshold, trans_init,
            o3d.pipelines.registration.TransformationEstimationPointToPoint()
        )
        
    print(f"[{title}] ICP Fitness: {reg_p2l.fitness:.4f}, RMSE: {reg_p2l.inlier_rmse:.4f}")
    print(f"[{title}] Correction Matrix:\n{reg_p2l.transformation}")
    
    # è½¬æ¢å› numpy (N, 6) ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
    pcd_transformed = source.transform(reg_p2l.transformation)
    pts = np.asarray(pcd_transformed.points)
    clrs = np.asarray(pcd_transformed.colors)
    
    return reg_p2l.transformation, np.hstack((pts, clrs))


# ================= ä¸»ç¨‹åº =================

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    # 1. åŠ è½½æ ‡å®šç»“æœ
    # Eye-in-Hand: T_Graip_Cam (OpenCV default output: Camera in Gripper frame)
    # P_Grip = T_LC_LE * P_Cam
    T_LC_LE = load_calibration_matrix("left_eye_in_hand.npy")
    T_RC_RE = load_calibration_matrix("right_eye_in_hand.npy")
    
    # Eye-to-Hand: T_Base_Cam (OpenCV default output: Camera in Base frame)
    # P_Base = T_LB_H_raw * P_HeadCam
    T_LB_H_raw = load_calibration_matrix("head_base_to_left.npy")
    T_RB_H_raw = load_calibration_matrix("head_base_to_right.npy")
    
    # 2. åŠ è½½å†…å‚
    intrinsics = {}
    for name in ['head', 'left', 'right']:
        intrinsics[name] = load_intrinsics(name)

    # 3. åªå¤„ç†ç¬¬ä¸€å¸§
    color_files = sorted(glob.glob(os.path.join(DATA_ROOT, "colors", "*_head_camera.jpg")))
    if not color_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¸§æ•°æ®ï¼Œè¯·æ£€æŸ¥ data/colors æ–‡ä»¶å¤¹")
        return

    # å¤„ç†ç¬¬ä¸€å¸§
    f = color_files[2]
    basename = os.path.basename(f)
    seq_id = basename.split('_')[0]
    print(f"\nProcessing First Frame {seq_id} ...")

    # åŠ è½½ Pose
    try:
        with open(os.path.join(DATA_ROOT, "poses", f"{seq_id}_left.json"), 'r') as jf:
            pose_l_data = json.load(jf)['pose']
        with open(os.path.join(DATA_ROOT, "poses", f"{seq_id}_right.json"), 'r') as jf:
            pose_r_data = json.load(jf)['pose']
    except FileNotFoundError:
        print("  âš ï¸ å¯¹åº”çš„ Pose æ–‡ä»¶ç¼ºå¤±ï¼Œå°†è·³è¿‡ Eye-in-Hand æ‹¼æ¥")
        pose_l_data = None
        pose_r_data = None

    # Pose: T_Base_End (End-Effector in Base frame)
    # P_Base = T_Base_End * P_End
    T_LB_LE = pose_to_matrix(pose_l_data)
    T_RB_RE = pose_to_matrix(pose_r_data)

    clouds_local = {} # å­˜å‚¨å„è‡ªç›¸æœºåæ ‡ç³»ä¸‹çš„ç‚¹äº‘
    clouds_global = [] # å­˜å‚¨ç»Ÿä¸€åˆ° Head ç³»ä¸‹çš„ç‚¹äº‘
    frames_to_show = [] # å­˜å‚¨éœ€è¦æ˜¾ç¤ºçš„åæ ‡ç³»

    # Reference Dictionaries to store raw transforms for ICP correction
    T_HeadCam_Base_map = {} # 'left': T, 'right': T
    
    # --- å¤„ç† Head ---
    # Head Camera is Global Origin
    name = "head"
    c_path = os.path.join(DATA_ROOT, "colors", f"{seq_id}_{name}_camera.jpg")
    d_path = os.path.join(DATA_ROOT, "depths", f"{seq_id}_{name}_camera.npy")
    
    head_cloud_np = None # For ICP target
    
    if os.path.exists(c_path) and os.path.exists(d_path):
        rgb = cv2.imread(c_path)
        depth = np.load(d_path)
        fx, fy, cx, cy = intrinsics[name]
        pc = depth_to_point_cloud(depth, rgb, fx, fy, cx, cy)
        clouds_local[name] = pc
        clouds_global.append(pc)
        head_cloud_np = pc # Store for ICP
        frames_to_show.append((np.eye(4), "Head_Camera"))

    # --- å¤„ç† Left ---
    # ç›®æ ‡: P_Head
    # è·¯å¾„: Cam -> End (EyeInHand result) -> Base -> Head (EyeToHand result is Base->HeadCam)
    #
    # 1. P_End = T_End_Cam @ P_Cam  (T_End_Cam å°±æ˜¯ calibration result "left_eye_in_hand.npy")
    # 2. P_Base = T_Base_End @ P_End
    # 3. P_Head = inv(T_Base_Head) @ P_Base (T_Base_Head å°±æ˜¯ "head_base_to_left.npy")
    
    name = "left"
    c_path = os.path.join(DATA_ROOT, "colors", f"{seq_id}_{name}_wrist_camera.jpg")
    d_path = os.path.join(DATA_ROOT, "depths", f"{seq_id}_{name}_wrist_camera.npy")
    
    left_cloud_global_np = None
    
    if os.path.exists(c_path) and os.path.exists(d_path) and pose_l_data is not None:
        rgb = cv2.imread(c_path)
        depth = np.load(d_path)
        fx, fy, cx, cy = intrinsics[name]
        pc_local = depth_to_point_cloud(depth, rgb, fx, fy, cx, cy)
        clouds_local[name] = pc_local

        T_End_Cam = T_LC_LE  # calibration result is Cam->End (or End->Cam? Let's check debug result)
        # Debug script said: T_base_target = T_base_end @ T_calib @ T_cam_target
        # So T_calib IS T_End_Cam. Correct.
        
        T_Base_End = T_LB_LE
        
        # Head Calibration Result: "head_base_to_left.npy"
        # Debug script said: T_end_target = inv(T_base_end) @ T_calib @ T_cam_target
        # This implies T_calib was treated as T_Base_Cam to check the invariant.
        # And since that check failed (high std dev), this matrix is suspicious.
        # But mathematically it represents T_Base_HeadCam.
        T_Base_HeadCam = T_LB_H_raw 
        
        T_HeadCam_Base = np.linalg.inv(T_Base_HeadCam)
        T_HeadCam_Base_map['left'] = T_HeadCam_Base # Store for ICP
        
        # P_Head = T_HeadCam_Base * T_Base_End * T_End_Cam * P_Cam
        T_total = T_HeadCam_Base @ T_Base_End @ T_End_Cam
        
        # ä¸´æ—¶ä¿®å¤ï¼šå› ä¸º Head æ ‡å®šå¯èƒ½æœ‰ 12cm è¯¯å·®ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆæ‰‹åŠ¨è¡¥å¿ä¸€ä¸ªå¤§æ¦‚çš„ä½ç§»çœ‹çœ‹
        # æˆ–è€…æš‚æ—¶åªä¿¡èµ–å®ƒçš„æ—‹è½¬ï¼Œå¹³ç§»ç½®é›¶ï¼ˆå¦‚æœæ˜¯åœ¨è°ƒè¯•é˜¶æ®µï¼‰
        # T_total = T_Base_End @ T_End_Cam # åªçœ‹ Base ç³»ä¸‹çš„æ‹¼æ¥
        
        pc_global = transform_point_cloud(pc_local, T_total)
        clouds_global.append(pc_global)
        left_cloud_global_np = pc_global
        frames_to_show.append((T_total, "Left_Camera"))

    # --- å¤„ç† Right ---
    name = "right"
    c_path = os.path.join(DATA_ROOT, "colors", f"{seq_id}_{name}_wrist_camera.jpg")
    d_path = os.path.join(DATA_ROOT, "depths", f"{seq_id}_{name}_wrist_camera.npy")
    
    right_cloud_global_np = None
    
    if os.path.exists(c_path) and os.path.exists(d_path) and pose_r_data is not None:
        rgb = cv2.imread(c_path)
        depth = np.load(d_path)
        fx, fy, cx, cy = intrinsics[name]
        pc_local = depth_to_point_cloud(depth, rgb, fx, fy, cx, cy)
        clouds_local[name] = pc_local

        T_End_Cam = T_RC_RE
        T_Base_End = T_RB_RE
        T_Base_HeadCam = T_RB_H_raw
        
        T_HeadCam_Base = np.linalg.inv(T_Base_HeadCam)
        T_HeadCam_Base_map['right'] = T_HeadCam_Base # Store for ICP
        
        T_total = T_HeadCam_Base @ T_Base_End @ T_End_Cam
        
        pc_global = transform_point_cloud(pc_local, T_total)
        clouds_global.append(pc_global)
        right_cloud_global_np = pc_global
        frames_to_show.append((T_total, "Right_Camera"))

    # --- ICP Refinement Step ---
    print("\n" + "="*50)
    
    if USE_ICP_REFINEMENT:
        print("ğŸ› ï¸  ICP è‡ªåŠ¨æ ¡å‡†æ­¥éª¤ (ENABLED)")
    else:
        print("â­ï¸  ICP è‡ªåŠ¨æ ¡å‡†å·²è·³è¿‡ (DISABLED)")

    icp_clouds = []
    
    if USE_ICP_REFINEMENT and head_cloud_np is not None:
        icp_clouds.append(head_cloud_np)
        
        # 1. Left Refinement
        if left_cloud_global_np is not None and 'left' in T_HeadCam_Base_map:
            print("æ­£åœ¨æ ¡å‡† Left Arm (Eye-to-Base)...")
            T_icp_L, pcd_L_new = run_icp_refinement(left_cloud_global_np, head_cloud_np, title="Left->Head")
            icp_clouds.append(pcd_L_new)
            
            # è®¡ç®—æ–°çš„ T_HeadCam_Base
            # T_new = T_icp @ T_old
            T_HB_old = T_HeadCam_Base_map['left']
            T_HB_new = T_icp_L @ T_HB_old
            
            # åŸå§‹æ ‡å®šæ–‡ä»¶å­˜çš„æ˜¯ T_Base_Head (= inv(T_HB))
            T_BH_new = np.linalg.inv(T_HB_new)
            
            print(f"ğŸ‘‰ å»ºè®®ä¿®æ­£ 'head_base_to_left' çŸ©é˜µä¸º:\n{T_BH_new}")
            save_path = os.path.join(RESULTS_DIR, "head_base_to_left_refined_icp.npy")
            np.save(save_path, T_BH_new)
            np.savetxt(save_path.replace('.npy', '.txt'), T_BH_new, fmt='%.8f')
            print(f"   å·²ä¿å­˜å»ºè®®çŸ©é˜µåˆ°: {save_path} (.npy & .txt)")

        # 2. Right Refinement
        if right_cloud_global_np is not None and 'right' in T_HeadCam_Base_map:
            print("æ­£åœ¨æ ¡å‡† Right Arm (Eye-to-Base)...")
            T_icp_R, pcd_R_new = run_icp_refinement(right_cloud_global_np, head_cloud_np, title="Right->Head")
            icp_clouds.append(pcd_R_new)
            
            T_HB_old = T_HeadCam_Base_map['right']
            T_HB_new = T_icp_R @ T_HB_old
            
            T_BH_new = np.linalg.inv(T_HB_new)
            
            print(f"ğŸ‘‰ å»ºè®®ä¿®æ­£ 'head_base_to_right' çŸ©é˜µä¸º:\n{T_BH_new}")
            save_path = os.path.join(RESULTS_DIR, "head_base_to_right_refined_icp.npy")
            np.save(save_path, T_BH_new)
            np.savetxt(save_path.replace('.npy', '.txt'), T_BH_new, fmt='%.8f')
            print(f"   å·²ä¿å­˜å»ºè®®çŸ©é˜µåˆ°: {save_path} (.npy & .txt)")
            
        print("="*50 + "\n")
        
        # è¯¢é—®æ˜¯å¦æ˜¾ç¤º ICP ç»“æœ
        print("å‡†å¤‡æ˜¾ç¤º ICP ä¿®æ­£åçš„èšåˆæ•ˆæœ...")
        visualize_merged(icp_clouds, frames_list=[])
        
        # å…³é”®ä¿®å¤: æ›´æ–° clouds_global ä»¥ä¾¿åç»­çš„ Sampling ä½¿ç”¨ä¿®æ­£åçš„æ•°æ®
        if len(icp_clouds) > 1:
            print("ğŸ”„ å·²å°†ä¿®æ­£åçš„ ICP ç‚¹äº‘åº”ç”¨åˆ°åç»­åˆå¹¶æµç¨‹ä¸­")
            clouds_global = icp_clouds

    # --- DP3 é‡‡æ ·å¤„ç† (FPS/Voxel) ---

    final_points = None
    if clouds_global:
        # åˆå¹¶æ‰€æœ‰ç‚¹äº‘
        merged_cloud = np.vstack(clouds_global)
        
        print(f"\nâœ… åŸå§‹åˆå¹¶ç‚¹äº‘ç‚¹æ•°: {len(merged_cloud)}")
        
        # ä¸ºäº†ç»™ DP3 ä½¿ç”¨ï¼Œé€šå¸¸éœ€è¦é™é‡‡æ · (ä¾‹å¦‚ 4096 æˆ– 2048 ç‚¹)
        # ç›´æ¥ç”±å‡ åä¸‡ç‚¹åš FPS ä¼šéå¸¸æ…¢ï¼Œå»ºè®®å…ˆä½“ç´ ä¸‹é‡‡æ ·å† FPS
        
        pcd = numpy_to_o3d(merged_cloud)
        
        # 1. ç§»é™¤ç¦»ç¾¤ç‚¹ (ç»Ÿè®¡æ»¤æ³¢)
        pcd, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
        print(f"ğŸ§¹ å»å™ªåç‚¹æ•°: {len(pcd.points)}")
        
        # 2. ä½“ç´ ä¸‹é‡‡æ · (Voxel Grid) - å¿«é€Ÿå‡å°‘ç‚¹æ•°
        voxel_size = 0.005 # 5mm
        pcd_down = pcd.voxel_down_sample(voxel_size=voxel_size)
        print(f"ğŸ“¦ ä½“ç´ ä¸‹é‡‡æ ·({voxel_size}m)åç‚¹æ•°: {len(pcd_down.points)}")
        
        # 3. æœ€è¿œç‚¹é‡‡æ · (FPS) - ç¡®ä¿ç‚¹äº‘åˆ†å¸ƒå‡åŒ€
        TARGET_POINTS = 4096
        if len(pcd_down.points) > TARGET_POINTS:
            pcd_fps = pcd_down.farthest_point_down_sample(TARGET_POINTS)
            print(f"ğŸ¯ FPSé‡‡æ ·åç‚¹æ•°: {len(pcd_fps.points)}")
            final_pcd = pcd_fps
        else:
            final_pcd = pcd_down
            
        final_points = np.hstack((np.asarray(final_pcd.points), np.asarray(final_pcd.colors)))

    # --- å®æ—¶æ˜¾ç¤º ---
    
    # 1. æ˜¾ç¤ºå•ç‹¬çš„ç‚¹äº‘ (Local Coordinates)
    #    è¿™é‡Œåªèƒ½çœ‹æ¯ä¸ªç›¸æœºè‡ªå·±æ‹çš„æ•ˆæœï¼Œç”¨æ¥éªŒè¯å¯¹é½ã€å»ç•¸å˜å’Œå†…å‚æ˜¯å¦åŸºæœ¬æ­£ç¡®ï¼ˆæ¯”å¦‚å¢™è¦æ˜¯ç›´çš„ï¼‰
    print("å‡†å¤‡æ˜¾ç¤ºç”±äº [Head, Left, Right] å•ç‹¬çš„ç‚¹äº‘ï¼Œè¯·ä¾æ¬¡æŒ‰ 'Q' æŸ¥çœ‹ä¸‹ä¸€ä¸ª...")
    # visualize_clouds(clouds_local)
    
    # 2. æ˜¾ç¤ºåˆæˆåçš„ç‚¹äº‘ (Global Coordinates in Head Frame)
    if final_points is not None:
        print("\nâœ… æ­£åœ¨æ˜¾ç¤ºåˆæˆ+é‡‡æ ·åçš„ç‚¹äº‘ç»“æœ (Ready for DP3)...")
        # å°† final_points è½¬æ¢ä¸º O3D å¯¹è±¡
        # åŒæ—¶ä¼ å…¥ frame åˆ—è¡¨å¯è§†åŒ–
        visualize_merged([final_points], frames_list=frames_to_show)
    elif clouds_global:
        print("\nâœ… æ­£åœ¨æ˜¾ç¤ºåˆæˆç‚¹äº‘ (æ²¡æœ‰è¿›è¡Œé‡‡æ ·)...")
        visualize_merged(clouds_global, frames_list=frames_to_show)
    else:
        print("æ²¡æœ‰ç‚¹äº‘ç”Ÿæˆ")

if __name__ == "__main__":
    main()
