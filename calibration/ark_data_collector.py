#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import os
import sys
import json
import time
import threading
import numpy as np
import cv2
import yaml
from pathlib import Path

# Add project root to path
FILE = Path(__file__).resolve()
ROOT = FILE.parents[1]  # parents[1] æŒ‡å‘é¡¹ç›®æ ¹ç›®å½• ROS2_AC-one_Play
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# Add act directory to path (so 'utils' can be imported directly)
ACT_DIR = ROOT / "act"
if str(ACT_DIR) not in sys.path:
    sys.path.insert(0, str(ACT_DIR))

# Load local message definitions & setup paths
from act.utils.setup_loader import setup_loader
# è¿™é‡Œçš„ msg ç›®å½•å…¶å®åœ¨ act/msg ä¸‹ï¼Œæ‰€ä»¥åº”è¯¥ä¼ å…¥ ACT_DIR
setup_loader(ACT_DIR)

from act.utils.ros_operator import RosOperator, Rate

import rclpy


# ================= é…ç½® =================
DATA_ROOT = "data_points_test"
SAVE_DIR_COLORS = os.path.join(DATA_ROOT, "colors")
SAVE_DIR_DEPTHS = os.path.join(DATA_ROOT, "depths")
SAVE_DIR_POSES = os.path.join(DATA_ROOT, "poses")
SAVE_DIR_INTRINSICS = os.path.join(DATA_ROOT, "intrinsics")

def load_yaml(yaml_file):
    try:
        with open(yaml_file, 'r', encoding='utf-8') as file:
            return yaml.safe_load(file)
    except Exception as e:
        print(f"Error loading yaml: {e}")
        return None

class ark_collector_args:
    """Mock args object for RosOperator"""
    def __init__(self):
        # é…ç½®æ–‡ä»¶è·¯å¾„æ›´æ­£ï¼šåœ¨ act/data/config.yaml è€Œä¸æ˜¯ data/config.yaml
        self.config = os.path.join(ROOT, 'act/data/config.yaml')
        self.camera_names = ['head', 'left_wrist', 'right_wrist']
        self.use_depth_image = True  # å¯ç”¨æ·±åº¦å›¾
        self.use_base = False
        self.record = 'Distance'
        self.frame_rate = 30
        self.ckpt_dir = '' 
        self.ckpt_name = ''
        self.episode_path = ''

def main():
    # 0. å‡†å¤‡ä¿å­˜ç›®å½•
    for dir_path in [SAVE_DIR_COLORS, SAVE_DIR_DEPTHS, SAVE_DIR_POSES, SAVE_DIR_INTRINSICS]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

    # 1. åˆå§‹åŒ– ROS & RosOperator
    rclpy.init()
    
    args = ark_collector_args()
    config = load_yaml(args.config)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ã€‚")
        return

    print("æ­£åœ¨åˆå§‹åŒ– Ark Robot Operator...")
    ros_operator = RosOperator(args, config, in_collect=True)
    
    # å¯åŠ¨ ROS spin çº¿ç¨‹
    def _spin_loop(node):
        while rclpy.ok():
            rclpy.spin_once(node, timeout_sec=0.001)

    spin_thread = threading.Thread(target=_spin_loop, args=(ros_operator,), daemon=True)
    spin_thread.start()

    # é¢„çƒ­ï¼Œç­‰å¾…æ•°æ®æµå»ºç«‹
    print("ç­‰å¾…è®¾å¤‡æ•°æ®æµé¢„çƒ­ (2s)...")
    time.sleep(2.0)

    print("\nâœ… æ–¹èˆŸæ•°æ®é‡‡é›†ç³»ç»Ÿå·²å°±ç»ªï¼")
    print("ğŸ“· ç›¸æœº: head, left_wrist, right_wrist")
    print("ğŸ® æ§åˆ¶:")
    print("   [Space]: å¼€å§‹/æš‚åœå½•åˆ¶ (10Hz)")
    print("   [Q]: é€€å‡ºç¨‹åº")
    print("-" * 50)

    # ä¿å­˜ç›¸æœºå†…å‚ï¼ˆç”¨äºåç»­ç‚¹äº‘ç”Ÿæˆï¼‰
    intrinsics_saved = False
    camera_names = ['head', 'left_wrist', 'right_wrist']

    count = 0
    recording = False
    last_save_time = 0
    SAVE_INTERVAL = 0.1  # 10Hz

    try:
        while rclpy.ok():
            # è·å–è§‚æµ‹æ•°æ®
            obs = ros_operator.get_observation()
            
            if obs is None:
                time.sleep(0.01)
                continue

            # === ä¿å­˜ç›¸æœºå†…å‚ï¼ˆåªéœ€ä¿å­˜ä¸€æ¬¡ï¼Œç”¨äºç‚¹äº‘ç”Ÿæˆï¼‰===
            if not intrinsics_saved and config:
                for cam_name in camera_names:
                    cam_key = cam_name + '_camera' if cam_name != 'head' else 'head_camera'
                    if cam_key in config:
                        intrinsics = {
                            'fx': config[cam_key]['camera_matrix']['data'][0],
                            'fy': config[cam_key]['camera_matrix']['data'][4],
                            'cx': config[cam_key]['camera_matrix']['data'][2],
                            'cy': config[cam_key]['camera_matrix']['data'][5],
                            'distortion': config[cam_key]['distortion_coefficients']['data'],
                            'width': config[cam_key]['image_width'],
                            'height': config[cam_key]['image_height']
                        }
                        intrinsics_path = os.path.join(SAVE_DIR_INTRINSICS, f"{cam_name}_intrinsics.json")
                        with open(intrinsics_path, 'w') as f:
                            json.dump(intrinsics, f, indent=2)
                intrinsics_saved = True
                print("âœ… ç›¸æœºå†…å‚å·²ä¿å­˜")
                
                # è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
                print(f"   obs keys: {obs.keys()}")
                if 'images' in obs:
                    print(f"   RGB cameras: {list(obs['images'].keys())}")
                if 'images_depth' in obs:
                    print(f"   Depth cameras: {list(obs['images_depth'].keys())}")
                else:
                    print(f"   âš ï¸  'images_depth' ä¸åœ¨ obs ä¸­")

            # === å‡†å¤‡æ˜¾ç¤ºå’Œä¿å­˜æ•°æ® ===
            displays_rgb = []
            displays_depth = []
            capture_data = {}  # {camera_name: (rgb, depth)}
            
            for cam_name in camera_names:
                # RGB
                if cam_name in obs['images']:
                    rgb_img = obs['images'][cam_name]
                else:
                    rgb_img = None
                
                # Depth (æ³¨æ„ï¼šobs é‡Œçš„ key æ˜¯ 'images_depth'ï¼Œä¸æ˜¯ 'depth_images')
                depth_img = None
                if 'images_depth' in obs and cam_name in obs['images_depth']:
                    depth_img = obs['images_depth'][cam_name]
                
                # æ˜¾ç¤ºé€»è¾‘ï¼šRGB + Depth å¯è§†åŒ–
                if rgb_img is not None:
                    # RGB é¢„è§ˆ
                    small_rgb = cv2.resize(rgb_img, (320, 240))
                    if recording:
                        cv2.circle(small_rgb, (300, 20), 8, (0, 0, 255), -1)  # çº¢ç‚¹è¡¨ç¤ºå½•åˆ¶ä¸­
                    
                    # æ˜¾ç¤ºç›¸æœºåç§°å’ŒçŠ¶æ€
                    display_name = cam_name.replace('_wrist', '')
                    status_color = (0, 255, 0) if depth_img is not None else (0, 165, 255)
                    status_text = f"{display_name} [D+]" if depth_img is not None else f"{display_name} [D-]"
                    cv2.putText(small_rgb, status_text, (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
                    displays_rgb.append(small_rgb)
                    
                    # Depth å¯è§†åŒ–
                    if depth_img is not None:
                        # å›ºå®šæ·±åº¦èŒƒå›´è¿›è¡Œå¯è§†åŒ–ï¼ˆé¿å…åŠ¨æ€å½’ä¸€åŒ–å¯¼è‡´çš„è·³å˜ï¼‰
                        # è®¾ç½®æ·±åº¦èŒƒå›´ï¼š0mm - 1000mm
                        DEPTH_MIN = 0  # mm
                        DEPTH_MAX = 1000  # mm

                        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
                        depth_clipped = np.clip(depth_img, DEPTH_MIN, DEPTH_MAX)
                        
                        # å½’ä¸€åŒ–åˆ°0-255
                        depth_normalized = ((depth_clipped - DEPTH_MIN) / (DEPTH_MAX - DEPTH_MIN) * 255).astype(np.uint8)
                        
                        # åº”ç”¨ä¼ªå½©è‰²
                        depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        small_depth = cv2.resize(depth_colored, (320, 240))
                        
                        # æ·»åŠ æ·±åº¦ç»Ÿè®¡ä¿¡æ¯
                        valid_depth = depth_img[depth_img > 0]
                        if len(valid_depth) > 0:
                            depth_mean = int(valid_depth.mean())
                            depth_std = int(valid_depth.std())
                            depth_text = f"Mean:{depth_mean}mm Std:{depth_std}mm"
                        else:
                            depth_text = "NO VALID DEPTH"
                        
                        cv2.putText(small_depth, depth_text, (5, 220), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                        displays_depth.append(small_depth)
                        
                        # ä¿å­˜æ•°æ®
                        capture_data[cam_name] = (rgb_img, depth_img)
                    else:
                        # æ— æ·±åº¦å›¾ï¼Œæ˜¾ç¤ºé»‘å±
                        black_depth = np.zeros((240, 320, 3), dtype=np.uint8)
                        cv2.putText(black_depth, "NO DEPTH", (80, 120), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                        displays_depth.append(black_depth)
                else:
                    # RGB ç¼ºå¤±ï¼Œæ˜¾ç¤ºé»‘å±
                    black_rgb = np.zeros((240, 320, 3), dtype=np.uint8)
                    cv2.putText(black_rgb, f"{cam_name} NO RGB", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
                    displays_rgb.append(black_rgb)
                    
                    black_depth = np.zeros((240, 320, 3), dtype=np.uint8)
                    displays_depth.append(black_depth)

            # æ˜¾ç¤ºé¢„è§ˆï¼šä¸Šæ’RGBï¼Œä¸‹æ’Depth
            if displays_rgb and displays_depth:
                row_rgb = np.hstack(displays_rgb)
                row_depth = np.hstack(displays_depth)
                combined = np.vstack([row_rgb, row_depth])
                cv2.imshow("Ark Data Collector Preview", combined)

            key = cv2.waitKey(10) & 0xFF
            if key == ord('q'):
                break
            elif key == ord(' '):
                recording = not recording
                if recording:
                    print(f"\nâ–¶ï¸  å¼€å§‹å½•åˆ¶... (ç›®æ ‡ 10Hz, ä¸‹ä¸€å¸§: {count})")
                else:
                    print(f"\nâ¸ï¸  æš‚åœå½•åˆ¶. (å·²ä¿å­˜: {count} å¸§)")

            # === è‡ªåŠ¨é‡‡é›†é€»è¾‘ ===
            if recording:
                current_time = time.time()
                
                # æ£€æŸ¥æ—¶é—´é—´éš” (10Hz)
                if current_time - last_save_time >= SAVE_INTERVAL:
                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼šæ‰€æœ‰ç›¸æœºéƒ½æœ‰æ•°æ®
                    if len(capture_data) == len(camera_names):
                        # è·å–æœºæ¢°è‡‚ä½å§¿
                        # obs['eef'] = [left_7d, right_7d] = [x,y,z,r,p,y,gripper] * 2
                        pose_left = obs['eef'][0:7].tolist()   # [x,y,z,r,p,y,gripper]
                        pose_right = obs['eef'][7:14].tolist()
                        
                        # ä¿å­˜æ‰€æœ‰ç›¸æœºçš„ RGB å’Œ Depth
                        for cam_name, (rgb, depth) in capture_data.items():
                            prefix = f"{count:04d}_{cam_name}"
                            
                            # RGB (BGRæ ¼å¼)
                            cv2.imwrite(os.path.join(SAVE_DIR_COLORS, f"{prefix}_camera.jpg"), rgb)
                            
                            # Depth (åŸå§‹uint16æ·±åº¦å€¼ï¼Œå•ä½ï¼šæ¯«ç±³ï¼Œä¸RGBå¯¹é½)
                            # å¯ç”¨äºç‚¹äº‘ç”Ÿæˆï¼šdepth_m = depth_array / 1000.0
                            depth_path = os.path.join(SAVE_DIR_DEPTHS, f"{prefix}_camera.npy")
                            np.save(depth_path, depth.astype(np.uint16))
                        
                        # ä¿å­˜ä½å§¿
                        with open(os.path.join(SAVE_DIR_POSES, f"{count:04d}_left.json"), 'w') as f:
                            json.dump({
                                "pose": pose_left[:6],  # [x,y,z,r,p,y]
                                "gripper": pose_left[6],
                                "unit": "m, rad"
                            }, f, indent=2)
                        
                        with open(os.path.join(SAVE_DIR_POSES, f"{count:04d}_right.json"), 'w') as f:
                            json.dump({
                                "pose": pose_right[:6],
                                "gripper": pose_right[6],
                                "unit": "m, rad"
                            }, f, indent=2)
                        
                        freq = 1.0 / (current_time - last_save_time)
                        print(f"\r[REC] âœ… Saved Frame {count:04d} | Freq: {freq:.1f}Hz", end="")
                        
                        count += 1
                        last_save_time = current_time
                    else:
                        # æ•°æ®æœªå¯¹é½ï¼Œç­‰å¾…ä¸‹ä¸€å¸§
                        pass
                        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    finally:
        print(f"\n\nğŸ“Š é‡‡é›†å®Œæˆï¼å…±ä¿å­˜ {count} å¸§æ•°æ®")
        print(f"   RGB: {SAVE_DIR_COLORS}")
        print(f"   Depth (uint16, mm): {SAVE_DIR_DEPTHS}")
        print(f"   Poses: {SAVE_DIR_POSES}")
        print(f"   Intrinsics: {SAVE_DIR_INTRINSICS}")
        print(f"\nğŸ’¡ ç”Ÿæˆç‚¹äº‘ç¤ºä¾‹:")
        print(f"   depth = np.load('depths/0000_head_camera.npy')")
        print(f"   depth_m = depth.astype(float) / 1000.0  # è½¬æ¢ä¸ºç±³")
        
        ros_operator.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()
        spin_thread.join(timeout=2.0)

if __name__ == '__main__':
    main()
